I- Spark ArchitectureAs we can see that Spark follows Master-Slave architecture where we have one central coordinator and multiple distributed worker nodes.The central coordinator is called Spark Driver and it communicates with all the Workers.Each Worker node consists of one or more Executor(s) who are responsible for running the Task.Executors register themselves with Driver.The Driver has all the information about the Executors at all the time.Spark Application = Driver + WorkersThe Spark Application is launched with the help of the Cluster Manager.Cluster manager can be any one of the following:Spark Standalone ModeYARNMesosKubernetesDRIVERDriver is a Java process. This is the process where the main() method of our Scala, Java, Python program runs. It executes the user code and creates a SparkSession or SparkContext and the SparkSession is responsible to create DataFrame, DataSet, RDD, execute SQL, perform Transformation & Action, etc.Responsibility of DRIVERThe main() method of our program runs in the Driver process. It creates SparkSession or SparkContext.Conversion of the user code into Task (transformation and action). It looks at the user code and determines are the possible Tasks, i.e. the number of tasks to be performed is decided by the Driver. But how does it determines the Tasks? – With the help of Lineage. (Discussed later in the blog)Helps to create the Lineage, Logical Plan and Physical Plan.Wondering what they are? Check my blog by clicking here.Once the Physical Plan is generated, the Driver schedules the execution of the tasks by coordinating with the Cluster Manager.Coordinates with all the Executors for the execution of Tasks. It looks at the current set of Executors and schedules our tasks.Keeps track of the data (in the form of metadata) which was cached (persisted) in Executor’s (worker’s) memory.EXECUTOR:Executor resides in the Worker node. Executors are launched at the start of a Spark Application in coordination with the Cluster Manager.They are dynamically launched and removed by the Driver as per required.Responsibility of EXECUTORTo run an individual Task and return the result to the Driver.It can cache (persist) the data in the Worker node.Thinking how these Driver and Executor Processes are launched after submitting a job (spark-submit)?Well, then let’s talk about the Cluster Manager.CLUSTER MANAGERSpark is dependent on the Cluster Manager to launch the Executors and also the Driver (in Cluster mode).We can use any of the Cluster Manager (as mentioned above) with Spark i.e. Spark can be run with any of the Cluster Manager.Spark provides a script named “spark-submit” which helps us to connect with a different kind of Cluster Manager and it controls the number of resources the application is going to get i.e. it decides the number of Executors to be launched, how much CPU and memory should be allocated for each Executor, etc.Working Processspark-submit –master –executor-memory 2g –executor-cores 4 WordCount-assembly-1.0.jarLet’s say a user submits a job using “spark-submit”.“spark-submit” will in-turn launch the Driver which will execute the main() method of our code.Driver contacts the cluster manager and requests for resources to launch the Executors.The cluster manager launches the Executors on behalf of the Driver.Once the Executors are launched, they establish a direct connection with the Driver.The driver determines the total number of Tasks by checking the Lineage.The driver creates the Logical and Physical Plan.Once the Physical Plan is generated, Spark allocates the Tasks to the Executors.Task runs on Executor and each Task upon completion returns the result to the Driver.Finally, when all Task is completed, the main() method running in the Driver exits, i.e. main() method invokes sparkContext.stop().Finally, Spark releases all the resources from the Cluster Manager.LINEAGE:When a new RDD is derived from an existing RDD using transformation, that new RDD contains a pointer to the parent RDD and Spark keeps track of all the dependencies between these RDDs using a component called the Lineage. In case of data loss, this lineage is used to rebuild the data. DataFrame, DataSet, SQL are internally converted to RDDs for computation as RDDs are the lowest level of abstraction in Spark. So, all the transformations that are involved internally in a DataFrame, DataSet, SQL can be seen by converting them to RDD.The difference between DataFrame and RDDA DataFrame in Apache Spark is built on top of an RDD by adding a schema, which is a description of the structure of the data, including the names and types of the columns. The schema allows Spark to optimize the processing of the data, such as by using column pruning and predicate pushdown, as well as to provide a more convenient API for working with structured data.When you create a DataFrame, Spark takes the RDD as input and applies the schema to it, creating a new RDD that is now a DataFrame. The new RDD contains the same data as the original RDD, but the data is now organized into named columns with specific data types.Cluster mode vs Client mode:YARN client mode: In this mode, the Spark driver runs on the client machine where the application was submitted. The client communicates with the YARN ResourceManager to request containers to run the application's executors on worker nodes. In client mode, the client's network connectivity determines the network connectivity of the Spark executors.Use cases : useful for debugging and troubleshooting simplify deployment, but if experiences network connectivity issues, the Spark application will fail or experience delays and all data must be transmitted between the cluster and the client machine.YARN cluster mode: In this mode, the Spark driver runs inside a YARN container on a worker node in the cluster. The driver is managed by YARN, and the application's executors are also launched as YARN containers on the worker nodes. In cluster mode, the Spark executors have direct network connectivity to each other and to other services in the YARN cluster.Use cases : lead to faster execution times for large-scale data processing jobs cluster mode is recommended for large-scale data processing jobs where efficient resource utilization and fault toleranceHow YARN works:YARN (Yet Another Resource Negotiator) A framework of generic resource management for distributed workloads schedules jobs to run on those resources. YARN consists of a ResourceManager, which manages the overall allocation of resources, and NodeManagers, which manage the resources available on each machine in the cluster. When running Spark on YARN, each Spark executor runs as a YARN container.How to configure the job resources:The resources required for a job can be configured using the YARN configuration files, such as yarn-site.xml, mapred-site.xml, and hdfs-site.xml. These files allow you to specify the number of nodes, the amount of memory and CPU required, and other job-specific settings.–executor-cores / spark. executor. ...–executor-memory / spark. executor. ...–num-executors / spark. executor. ...Total Number Executor = Total Number Of Cores / 5 => 90/5 = 18.Overhead Memory = max(384 , 0.1 * 21) ~ 2 GB (roughly)Heap Memory = 21 – 2 ~ 19 GB.Case 1 Hardware – 6 Nodes and each node have 16 cores, 64 GB RAM First on each node, 1 core and 1 GB is needed for Operating System and Hadoop Daemons, so we have 15 cores, 63 GB RAM for each nodeWe start with how to choose number of cores:Number of cores = Concurrent tasks an executor can runSo we might think, more concurrent tasks for each executor will give better performance. But research shows that any application with more than 5 concurrent tasks, would lead to a bad show. So the optimal value is 5.This number comes from the ability of an executor to run parallel tasks and not from how many cores a system has. So the number 5 stays same even if we have double (32) cores in the CPUNumber of executors:Coming to the next step, with 5 as cores per executor, and 15 as total available cores in one node (CPU) – we come to 3 executors per node which is 15/5. We need to calculate the number of executors on each node and then get the total number for the job.So with 6 nodes, and 3 executors per node – we get a total of 18 executors. Out of 18 we need 1 executor (java process) for Application Master in YARN. So final number is 17 executorsThis 17 is the number we give to spark using –num-executors while running from spark-submit shell commandMemory for each executor:From above step, we have 3 executors per node. And available RAM on each node is 63 GBSo memory for each executor in each node is 63/3 = 21GB.However small overhead memory is also needed to determine the full memory request to YARN for each executor.The formula for that overhead is max(384, .07 * spark.executor.memory)Calculating that overhead: .07 * 21 (Here 21 is calculated as above 63/3) = 1.47Since 1.47 GB > 384 MB, the overhead is 1.47Take the above from each 21 above => 21 – 1.47 ~ 19 GBSo executor memory – 19 GBFinal numbers – Executors – 17, Cores 5, Executor Memory – 19 GBWhat is Cluster Manager and Executor:In the context of Spark, a cluster manager is a component that manages the resources of a Spark cluster, while an executor is a component that runs on a node in the cluster and executes tasks on behalf of the driver.Difference between Action et Transformation:In Spark, transformations are operations that create a new RDD from an existing one, while actions are operations which return a value to the driver program after running a computation on the dataset.ReduceByKey VS GroupByKey:reduceByKey est en fait plus performant que groupByKey lorsqu'il y a beaucoup de donnéesPrendre en compte le shuffle lorsque des traitements parallèles sont mis en place, notamment les opérations comme reduceByKey et groupByKey qui en produisent.Préférer reduceByKey à groupByKey pour avoir une meilleure performance dans le temps de traitement et moins de problèmes de mémoire (en particulier si le nombre moyen d'éléments par clé par partition est élevé).Si vous voulez aller plus loin il y a combineByKey qui offre encore plus d'options (reduceByKey est implémenté en utilisant combineByKey)What is Shuffle and when?In Spark, a shuffle is a process of redistributing data across the nodes of a cluster to perform operations such as grouping, sorting, and joining. Shuffles can be expensive in terms of performance and should be minimized whenever possible. Shuffles occur when there is a change in the partitioning of the data, such as a groupByKey operation or a join operation.How to apply UDF on SQL DataFrame ?UDF’s are the most expensive operations hence use them only you have no choice and when essential.In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.Coalesce VS RepartitionThe repartition algorithm does a full shuffle of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a full shuffle.What is Broadcast variable ? When use it ?In Spark, a broadcast variable is a read-only variable that is cached and available on all nodes in the cluster for efficient access. Broadcast variables are used to share a small amount of data, such as lookup tables or other metadata, with all of the tasks running on the cluster.The advantage of using a broadcast variable is that it avoids the overhead of sending data over the network multiple times. Instead, the data is sent once and cached on each node in the cluster, reducing network traffic and improving performance.A broadcast variable is created using the SparkContext.broadcast() method, which takes a value that will be broadcasted to all nodes in the cluster. Once a broadcast variable is created, it can be used in Spark transformations and actions.Broadcast variables are useful in scenarios where the same data is used repeatedly in Spark tasks, and the data is small enough to fit in memory. For example, a lookup table can be broadcasted to all nodes in the cluster, which can then be used by each task to perform lookups without the need to send the entire table over the network for each lookup.Broadcast variables should not be used for large data sets, as this can cause memory issues on the nodes in the cluster. Additionally, broadcast variables are read-only, so they cannot be updated once they are created.How can you Minimise the data shuffle in join in Spark?If you have to do an operation before the join that requires a shuffle, such as aggregateByKey or reduceByKey , you can prevent the shuffle by adding a hash partitioner with the same number of partitions as an explicit argument to the first operation before the join.To minimize data shuffling in Spark joins, you can follow these best practices:Co-partition the DataFrames: Co-partitioning the DataFrames on the join key can reduce the amount of data shuffling needed. When two DataFrames have the same partitioning scheme, Spark can perform the join locally, without the need to shuffle the data across the network.Use broadcast joins: If one of the DataFrames is small enough to fit in memory, you can use a broadcast join. A broadcast join involves broadcasting the small DataFrame to all nodes in the cluster and then joining it with the larger DataFrame. This reduces data shuffling because the smaller DataFrame is only broadcasted once, rather than being shuffled across the network.Use bucketing: Bucketing involves partitioning the DataFrames into buckets based on the join key. This can reduce data shuffling because data with the same join key will be in the same bucket, making it easier for Spark to perform the join locally. However, bucketing requires careful planning and may not be suitable for all scenarios.Use column pruning: Column pruning involves selecting only the necessary columns from each DataFrame before performing the join. This reduces the amount of data that needs to be shuffled across the network and can improve performance.Use proper partitioning: Ensure that the DataFrames are properly partitioned before performing the join. A good partitioning scheme can reduce the amount of data shuffling required and improve performance.Methods of persist : Persist() Vs Cache()The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY), i.e. cache is merely persist with the default storage level MEMORY_ONLYBut Persist() We can save the intermediate results in 5 storage levels.MEMORY_ONLYMEMORY_AND_DISKMEMORY_ONLY_SERMEMORY_AND_DISK_SERDISK_ONLYSER = object to bin then transfert in a network then get pack back to the identical object => Can be transfert between tasksHow Spark divide Job into stages, Dags, tasks ?Whenever there is a shuffling of data over the network, Spark divides the job into multiple stages. Therefore, a stage is created when the shuffling of data takes place. These stages can be either processed parallelly or sequentially depending upon the dependencies of these stages between each other.What is the difference between tasks and threads ?In spark is reared to unit of work (unit of parallelism)1 process = multiple threads (concurrent) thread can communicate share the same memory space1 task = 1 process = Many threadsCommunication between process needs serialisation ⇒ justifiable only if there is a lower cost over 1 processWhat is Apache Parquet?Apache Parquet is a binary file format that stores data in a columnar fashion. Data inside a Parquet file is similar to an RDBMS style table where you have columns and rows. But instead of accessing the data one row at a time, you typically access it one column at a time.Apache Parquet is one of the modern big data storage formats. It has several advantages, some of which are:Columnar storage: efficient data retrieval, efficient compression, etc...Metadata is at the end of the file: allows Parquet files to be generated from a stream of data. (common in big data scenarios)Supported by all Apache big data productsWhat is the difference between Scala and parquet:Both CSV and Parquet formats have their advantages and disadvantages. Here are some considerations for each format:CSV:Advantages:Simple and widely used format that can be easily imported into many tools and systems.Good for small to medium-sized datasets with simple schema and no complex data types.Human-readable and editable using text editors or spreadsheet programs.Disadvantages:Can be slow and memory-intensive to read and write, especially for large datasets with many columns.Does not support nested data structures or advanced compression methods, which can increase storage and I/O costs.Parquet:Advantages:Columnar storage format that is optimised for analytics workloads with large datasets and complex data types.Supports nested data structures, which can reduce storage costs and improve query performance.Uses efficient compression algorithms, which can save storage space and reduce I/O costs.Supports efficient reading and writing of subsets of data, which can improve query performance and reduce network bandwidth.Disadvantages:Not as widely supported by all tools and systems as CSV.More complex and less human-readable than CSV.