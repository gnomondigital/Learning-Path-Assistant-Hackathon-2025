IntroductionIn the previous Learning Path lesson we’ve seen how does supervised Machine Learning works. And in this course we will talk about unsupervised Machine Learning. The main difference between those two chapters are about how to classify data when you don't have any output to provide.We’re gonna use again Python’s librairie scikit-learn to predict, matplotlib to plot graphs and numpy & pandas to create datasets . To install it run those commands in your terminal :Then to ensure it’s correctly installed run this in a Python file :If you have an error ModuleNotFoundError while executing this line refers to those websites : https://scikit-learn.org/stable/install.html for scikit-learn, https://stackoverflow.com/questions/18176591/importerror-no-module-named-matplotlib-pyplot for matplotlib,https://stackoverflow.com/questions/33481974/importerror-no-module-named-pandas for pandas,https://stackoverflow.com/questions/7818811/error-import-error-no-module-named-numpy-on-windows for numpy.The goal of this course is to train our data with only an input and cluster a set of value to predict a class for that set.Clustering the dataImagine we have a set of 5 000 points defined by their coordinates x and y like the image below :Dataset of points organised in 5 clustersIn this example we can see that the dataset of point have been organised by a computer into clusters, those clusters are the equivalent of class names in supervised classification. First we’re gonna generate a random dataset of points (with x, y coordinates) using make_blobs :And now X is like array([[-7.72642091, -8.39495682], [ 5.45339605, 0.74230537][-2.97867201, 9.55684617], … ]) and y is a list of the solution (aka the cluster label). We're gonna only X to train the data.make_blobs(n_samples=100, centers=3, n_features=2, random_state=42) mean we want a dataset of 100 points with 2 features each (X and Y coordinates) and random_state is there to make a random dataset repeatable. It returns a tuple of the dataset we wanted and the solution of the class they belong to.But like I said we’ll only train our model with the input and the computer will have to cluster every sets by himself. To cluster the set of coordinates we’ll use the K-Means package which is based on the k-means mathematical algorithm to work :Out : [1 0 2 0 1 0 2 0 0 2 2 1 1 2 2 1 1 2 1 1 2 1 1 2 2 2 0 1 1 1 1 0 0 1 2 2 2 2 0 0 1 2 0 2 2 0 1 1 1 0 0 0 2 1 1 1 2 2 0 2 1 0 1 0 1 1 0 1 0 0 0 1 1 2 0 1 0 1 0 0 2 0 2 1 2 2 2 0 2 0 0 0 2 0 2 2 2 0 1 2]After instantiating, our model k-means have to sort the data into three cluster, because I put in the script n_clusters=3. We’ll fit it on X only and predict in which cluster every points (generated with make_blob) belongs. Those cluster labels are stored in a list named y_pred.As a human being you can clearly spot in the above image that there is three clusters. And of course a computer cannot visually determine that. We’ll talk about how to determine the perfect number of clusters later. For the moment we’re agreeing that there’re 3 clusters.And we can plot the dataset of point, and by adding c = y_pred we can color the points according to their colors :As you can see the model K-Means has predicted correctly every class of every set of coordinates. To check the differences between how KMEANS sort the point and the true solution of they're sorted, run this :Determine the optimale number of clusterIn the previous chapter we already admit that the scatter had 3 clusters, but in a true situation we need to determine automatically ourselves the optimale number of clusters, for that we can use the inertia.The inertia represent the difficulty an unsupervised model has to cluster the data. In the example below you can see how the inertia evolve according to how we make n_cluster bigger. The optimal number of cluster in this case is 4.Out : 1 13983.0649381104232 8256.3401136701383 2835.8358966259264 356.975856105953545 327.697497235775366 304.78204968324437 280.453469842617448 261.76753874309519 246.6218845934524As you can see the inertia evolve depending of n_cluster, and once the optimal number of cluster is passed the inertia drastically drops, using this method we can easily select the correct number of cluster. This method is useful if you don't know the number of cluster you need, but if you have already decided you don't need it.Dimension reductionA problem might occurs when the dataset you wanna predict the label has too many columns, the columns doesn't have the same importance to determine in which cluster they belongs.This is why Dimension reduction is important if your dataset has too many columns, and one technique to permit an easier clustering is the PCA decomposition. The goal of Principal Component Analysis is to reduce the dimension (the columns) of the dataset without loosing informations. Like in this example :Out :[2 1] [3 2]]After : [[-1.03896057] [-0.15728598] [ 1.19624655]]Using n_components=1 we have now 1 dimension instead of two. The technique is useful when you have much more columns to deal with. Also here is an example with a dataset randomly created with 1000 lines of 10 features :As you can see the clustering is hard because they're too many features to take in consideration to cluster. But now if we're using PCA to transform the dataset into a smaller one, the clustering is easier :Explained variance by column [0.11672707 0.11221938]Cumulative Explained variance 0.22894644909036127Information percentage lost 0.7710535509096388With this dimension reduction using PCA it's easier for K-Means to cluster the data. One last word about PCA, how to choose the optimal number for n_components ?One of the method to make that choose is by calculating the Cumulative explained variance. PCA turns your n features into k features with k After using pca.transform function you get a new dataset of k dimensions where column 0 contains a lot of informations about the dataset before transform, the column 1 contain some remaining information from column 0, the column 2 some remaining information from column 0 and 1 etc… And ideally the amount of information stored in your transformed dataset from the previous is from 90% to 99%, the remaining few percent are considered not important. Here is an example of Cumulative explained variance :Explained variance ratio: [0.93449835 0.06134434]Cumulative explained variance ratio: [0.93449835 0.99584269]In this example by reducing the dimension of a 10 columns dataset to 2 columns, our two new columns own 11% each, which means we lost ~78% of the informations. If we want only 10% of the information lost maximum we can check the Cumulative Explained variance to determine the right number of component :“df” is a data frame that contains three columns X1, X2 and X3 for 1000 lines, and all of those columns are mathematically correlated. By reducing the number of columns by 3 to 2, the cumulative explained variance shows that PCA(n_components=2) had kept 99% of the informations and that PCA(n_components=1) could contains 93% of it.Anomaly DetectionSometimes when you will plot your cluster you'll found a few points far away from the others. Those points are anomalies. Anomaly means that whatever this point represents its behavior isn't the same as the others.Illustration of anomalies in a clusterDetecting those anomalies are very useful. For example to detect tax fraud, the tax services are using what you'll learn in this chapter. To detect anomalies we'll use the sklearn library IsolationForest at first with which we will try to discriminate anomalies from the regular points.In this exemple we created a scatter plot of an unique cluster and we will fit it to IsolationForest and predict to detect anomalies, IsolationForest.predict() return a list of 1 and -1, respectively for normal values and anomalies. Then second plot shows that every point too far away from the cluster's center is considered an anomaly.You can also adjust to tolerance to anomalies using the contamination parameter to make it more permissive.They're a few other sklearn package to predict anomalies, ensure to use the one with the optimal parameters that fits better to the situation. As a conclusion here is a script to illustrate how different they are :Unsupervised Machine Learning in a concrete caseIn this last part, we're gonna work on a more concrete case closer of what you could have as a real job. For that we can load a dataset directly from sklearn. This dataset will be about Californian housing information, this dataset can also be used for supervised ML.MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude 0 8.3252 41.0 6.984127 1.02381 322.0 2.555556 37.88 \1 8.3014 21.0 6.238137 0.97188 2401.0 2.109842 37.86 Longitude MedHouseVal 0 -122.23 4.526 1 -122.22 3.585The dataFrame inputs are a few houses characteristics of California, and the output the medium value of the house. For this exercise we will only use X, because we don't need the output for unsupervised ML.The goal of this exercise is to plot the abnormal Californian houses and illustrate how importante dimension reduction is importante in this operation. For this application we will arbitrarily ****expect that 0,1% of the houses aren't normal.In those plots we can see that IsolationForest have found some anomalies within the data, but as an human being it's hard to see in the second plot how those point could be an anomaly, even if it's probably right. But now let's try to reduce the dimension.[0.99978933 0.99990261][0.99978933 0.99990261]As you can see PCA(n_components=1) contains already 0.9997 of the X's informations, but for this exercice we'll keep a second composant to make it easier to plot.Value : -1, Count : 21Value : 1, Count : 20619After reducing the dimension we can see that even if the count of anomalies hasn't changed it's much easier for human's eyes to spot the anomalies.Note : It doesn't mean that the owners of the abnormal houses are frauder, the anomaly of their houses can just be that they own a mansion far away from the downtown. But as a tax inspector it would be a good idea to check those first.Here is the complete code →ConclusionUnsupervised Machine Learning is either a way to classify what Supervised ML cannot or a way to make the data relevant for Supervised ML. Also always remember that :X must be a list of list and model.predict() will be a list.Cluster the data using the cluster function you want but make sure the inertia is low.If you are overwhelmed with too much columns use dimension reduction but try to minimise the informations looses (>10% is great).Detect anomalies using the librairies we talked about and set the parameters according to what is your use case. Parameters explanation are available on the sklearn website, an example atsklearn.ensemble.IsolationForest