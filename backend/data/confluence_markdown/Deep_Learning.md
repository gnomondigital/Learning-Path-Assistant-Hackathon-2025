IntroductionWhat is a neural networkThis course talks about Deep Learning which is a supervised learning, but different of Machine Learning. Indeed Machine Learning uses mathematical algorithm to classify the data where Deep Learning mimics the behavior of the brain neural network and how neurons exchange informations between each other.Before even starting coding let's talk a little about what are a neuron and a perceptron. A neuron is to over-simplify a network of connected wire where electric signal travel within to treat informations in your brain.A perceptron is an artificial neuron connected to some other artificial neuron where integer or float travel within. A artificial neural network is a network of perceptron organized by layers, with an input and an output. The other layers are called hidden layers, and every perceptron is connected to every perceptron of the previous and the next layer.A neural network of 1 hidden layer, an input of size 3 and an output of size 2Now we can talk about a neuron behavior, and for that we will use a metaphor: how to cook a cake :So to make a cake you need to Mix Flour Salt and Baking powder, Mix Egg Cream and Sugar, and then Mix the two Mixing and then bake that final mix to get your cake. In this example the ingredients are the input, the cake the output, and the instructions the hidden layers. But also to succeed your cake you need to set the good amount of each ingredients (we'll use the arrow style as unity, the more it's big the more the ingredient weight must be hight).So for example you need a lot of flour and Sugar, but a little of salt and baking powder. A neural network works pretty much the same you have inputs values and every link has a weight :This picture represent how a random neurone treat the information, this Neurone has two inputs of value 1.1 and 2.5 and accord to those an importance of 0.1 and 0.23. The Neurone will do the sum of value*weight of his inputs and his output will be a function F of this sum, for example :The output will be for the neurones of the next layer is :So now a full neural network could looks like this (arrow's styles represent weights) :With input values that gets in and a complex combination of sum of different functions all linked with different weights who leads to outputs values.Now you're maybe asking how does the weights are getting set ? Well it's time to talk about what happens when you train a DL model. The model training is a succession of forward and back propagation. It would be easier to understand with an example. Let's say we have a dataset and we want to predict class Apple or Banana depending of it's skin color, we could have something like this :Red (input)Green (input)Blue (input)Apple (output)Banana (output)1001011001……………Now we train our dataset starting with the first line by putting as an input 1, 0, 0, it will propagate through the layers and give you an exit value, that's the forward propagation.In this over-simplified network by entering 0, 0, 1 we get 50%-50% because every path has the same weight. But we want the model when you ask what is 1, 0, 0 to answer “it's an apple”. So you tell the model the correct answer and it will adjust his weights by back propagation.And the weights will be updated by adding or subtracting the actual weight with the learning rate, now the network give more importance to the first neurone of the input to take a decision. And after this, it's forward propagation again with line 2 of the dataset :And so one, until the model have been trained on the entire dataset. Upon it's done the model will be able to recognize the fruit by it's color.That was the theory about how does a neural network works, now let's ensure you python environnement is working before doing practice.Install Keras & TensorflowFor this course we will need a python environment compatible with the librairies we'll use. The last version of python is 3.11 but Tensorflow is only compatible with python3.8 or 3.9. Type those commands :If you have an ModuleNotFound error you probably need to create a virtual environnement checkout this link : https://www.activestate.com/resources/quick-reads/virtual-python-shell/ and install python 3.9.2, or try with a jupyter notebook.If you have conda installed, Python 3.8.16 is a compatible version.Regression networkLet's start by creating a small neural network for regression. Using the numpy librairy, we can generate a X correlated to a y. To understand this script, here are some useful definitions :loss : how far your model's prediction are from the truth at some point.Sequential : the kind of neural network I was talking about in the introduction.Dense : the hidden layers of the neural network.activation : the function used on a perceptron after doing the sum of the weights times the values. Some example below :Epoch : how many time the model have to do forward and back propagation.Note: model.fit() save the metrics evolutions into a dictionary, which can be plot.The plot shows how far the model is from the truth as you can see the more epoch goes the more the model is close. The model must converge to a value (here 0.1) or else it's underfitted.As you can notice the more the model is training the more his prediction are close from the truth. For a better understanding I'll show you a picture that match the script with a schema.Note 1: default value for parameter activation is f(x) = xNote 2: input_shape=(4,) and not 4 because it must be of type tupleNow you're able to create a regression Deep learning model, let's try classification.Here are all the activations functions for the hidden layers :'relu' gives neural values between 0 and ∞'tanh' gives neural values between -1 and 1'linear' gives neural values between -∞ and ∞'elu' gives neural values between -const and ∞'selu' gives neural values between -const and ∞'softplus' gives neural values between 0 and ∞ (grow exponentially)'softsign' gives neural values between -1 and 1 (but with more attenuated negative values than tanh)LeakyReLU gives neural values between -∞ and ∞ ****(but with attenuated negative values)'sigmoid' gives neural values between 0 and 1'hard_sigmoid' gives neural values linear between 0 and 1'softmax' gives ****neural values between 0 and 1, used only for the exit hidden layer (highest values are exponentially higher than the ones below)'softmax_gumbel' gives neural values between 0 and ∞, used only for the output hidden layer (the more a value is eccentric the more it's attenuated)'exponential' gives neural values between 0 and ∞ (grow exponentially)The output values will be in the range of the last layer activation.Values Classification network (One dimension input)Predict a class using Deep Learning is quite close than predicting with Machine Learning. The main difference is the output shape. For this chapter we'll use the iris dataset, its purpose is to predict from four feature the species of an iris between 3.The dataset looks like this :sepal length in cmsepal width in cmpetal length in cmpetal width in cmspecies5.13.51.40.20 (Setosa)7.13.24.71.41 (Versicolour)6.33.362.52 (Virginica)……………With 4 inputs and 1 output, this dataset isn't optimal for Deep Learning. Indeed like it was said in the introduction : we need 1 input neurone per class. So we need to make a slight change to our dataset to make it looks like :sepal length in cmsepal width in cmpetal length in cmpetal width in cmsetosaversicolorevirginica5.13.51.40.21007.13.24.71.40106.33.362.5001…………………Now our dataset can fit into a 4 input/3 output neural network:Using this data we can create a classificator neural network, as the loss parameter we'll use the Categorical cross-entropy (this parameter is here to decide which method we'll use to determine the precision of the model, so for regression better use 'mse' and to classify in more than two class use 'categorical_crossentropy'. Also the output neuron function will be softmax to make the values between 0 and 1. How to optimize your network will be explained later.As you can see after been fitted on the dataset, all the weights of the neural network are set to predict in which class belongs any iris based on its features. Now it would give some better results if we could optimize all the parameters.Also if your goal is to labelize instead of classify, which means a set can belong to many classes use the exit activation : “sigmoid”.Hyper-parameters optimisationIn the previous we have seen the steps to create a fitted network :Sequential(), to instanciate an empty networkadd(), to add layers to our networkcompile(), to build our network according to what we addfit(), to train that modelAll of these functions take parameters, and choosing the good ones can make a huge difference. To decide which values for each parameters we can use the class GridSearchCV who will test every combo and gives us the best :This example only optimize a few parameters among all of those who can be optimized.Images Classification network (Many dimension input)Let's import a dataset open-source composed of plenty of handwritted digits from 0 to 9. X will be a list of image 8x8 pixels and y what number it represents.Therefore we're gonna try to create a neural network with an input_shape of (8, 8) and an exit of 10. Using image as the training data is very different of using values.When the input dimension is greater than 1, the neural network have to reduce it's dimension to match an 1D exit (a list of values between 0 and 1) without loosing informations, it's why we're gonna use a new features of tensorflow : MaxPooling and Conv. A combo of those two can create an image manager neural network.The neural network would looks like something like this :A digit image have three dimension : height, width and hue. It's why they are reshaped into 8 by 8 pixel, and the “1” in the code below at the asterix is there because the image is in black and white. If the digits images were in color their shapes would be (8, 8, 3). Because of the three color red, green, blue. But in this situation it's black and white and the third dimension represent only the blackness intensity. Also note that we are dividing every pixel hue by their maximal value because MaxPooling neural network works better with values between 0 and 1.For a better explanation here is a schema of the neural network we've created above. It shows how a three is fitted to the network. Note that the pixel's hue of grey represents int and aren't realistic :To be quick the succession of Conv and MaxPooling reduce the dimension without loosing too much informations, and at the end the output layer will be a regular 1D one.Usually we're using 3x3 kernel for Conv2D, if your image is larger than 128x128 you may use 5x5 kernel but it's not mandatory. It also must be an odd number.Now let's plot the result in a subplot :As you can see we have a few errors, sometimes a four looks like an eight. We could fix that using GridSearchCV.Prevent OverfittingIn this chapter we'll see all the solution to prevent overfitting. As a reminder : overfitting occurs when a models learn too much how to classify/regress the train data and cannot anymore generalize on other data. First look at this model :Note : on the .fit() function I've added verbose=0 to hide the training verboseThis script create a train dataset of 100 times the same small amount of data. That way the model will overfit, because it cannot generalize what it learns from the train set on the test set.As you can see in this picture the model is slightly overfitting. By ploting the training and test loss, we can spot that the model have great results on training set (blue), but the more he get fitted with the training set the less it's efficient on the testing set.To spot the overfitting you can check if :The train loss plot in converging, but the test loss plot is decreasing and then increasing.The train loss plot in converging, but the test loss plot keeps decreasing.They're a few methods to get ride of overfitting : the first one of course is using a bigger dataset with more new sets to train with.And also remember that if your neural network is too big for the data size (too much layers and/or too much neural) if will also overfit.But for now let's try to reduce overfitting without touching the input dataset.Add a Dropout to the modelThe Dropout is the most obvious solution to deal with overfitting. Adding a dropout in your model means that a set percentage of neurone will be ignored during the train. Therefore the effect is that the network becomes less sensitive to the specific weights of neurons. This results in a network capable of better generalization and less likely to overfit the training data.By adding one layer of Dropout we can see that the overfitting is slightly decreased (The orange curve isn't mainly increasing). Dropout can be added between every layers with a float value between 0≤xEarlyStopping is what we call a callback, their job is to affect model learning. And as the name suggests EarlyStopping will stop the training if one metrics of the model isn't decreasing or increasing anymore. Let's call EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10).monitor is the metrics the callback will supervise to decide to stop or not.mode='min' is to stop the train if the monitor value increase.verbose=1 is to print at which epoch the train will stop.patience=10 means how patiente the callback will be before stopping.So this line means : Stop the train if the test train loss is increasing during 10 epochs or more.Epoch 26: early stoppingUsing this callback we can prevent the model to increase too much. That callback is used to stop the model, callbacks have various utilities, feel free to check-up what are the others and what can they do for your model.Add L2 regularizers to the modelL2 regularization is a kernel regularizer that adds a penalty term proportional to the square of the weights to the loss function during neural network training.It encourages smaller weight values, which helps prevent overfitting.Using the L2 you can prevent neural weights to be too hight and more generic, which means they would fit more to the model. Usual values are between 0.1 and 0.001.Augment the dataIf the model is overfitting it's maybe because the dataset is too small for the model to generalize on new data. Data Augmentation is a process used for images model to create new images from the old ones.Using tensorflow.keras.layers functions we can automatically create infinity of new images from one. Checkout the website of Keras for parameters details. Augment your data will reduce an overfitting image model.Some other useful featuresSave and Load a modelWith the callback ModelCheckPoint you can save a model and load it to skip a long fitting process :........................] - ETA: 0s - loss: 0.4893 Epoch 30: loss improved from 0.44678 to 0.43834, saving model to models/my_model.h5 5/5 [==============================] - 0s 2ms/step - loss: 0.4383 1/1 [==============================] - 0s 17ms/step]]>monitor='loss', save the model if the loss decrease or increasemode='min', decide if we save the model depending if the monitor decrease or increasesave_best_only=True, save the model only if the monitor is better than the previous epochverbose=1, activate verboseAnd after you can load it and predict :Use of to_categoricalInstead of using pd.get_dummies you can use to_categorical to turn your output into numpy.array instead of pandas.DataFrame. [[1. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1.]] ]]>Cross ValidationTo efficiently evaluate a monitor of your model, it's common to use a cross validation. To do that you need to split your dataset into sub-dataset and evaluate the performances for each of them and get the mean as a result:Batch the train setTo save time and ressources during the fitting you can add a parameter batch_size. By batching the model by 32 it will learn from the data 32 set by 32 set. For training on huge datasets it can save your precious time.Transfer LearningLast Chapter, we're gonna talk about another way to create a model : downloading an empty one and training it yourself (a.k.a. fine-tuning). Indeed http://Keras.io gives us everything we needs to skip the part where we need to decide which layer we'll use. Visit https://keras.io/api/applications/ for all the model available. For now let's try to create a model VGG16 using transfer learning. VGG16 require image of 32x32x3 or bigger, we'll use a dataset from tensorflow. It contains some images of animals and vehicles.Note : Like i said in the previous chapter an image in color is shape like width by length by color, with color like : [red_hue, green_hue, blue_hue], the combinaison of those three hue gives each pixels its color.Now we have our data we can import the model and train it :Note: I used optimizer=Adam(learning_rate=0.001) instead of optimizer='adam', using this last option set the parameters of Adam to default, if I want another learning_rate I can tune it that way.The transferred model and very large, so they don't need to be trained on a lot of epochs but each epoch might take a few minutes. Transfer Learning strategy looks like this:The left part is what we've done since done. Let's focus on the right one, we just used Strategy 1. The hidden layers are frozen, and we replaced the exit layer by another one with the good shape. Now let's try to predict :The model isn't perfect yet but it's good start, using GridSearchCV we could optimize it, all the model from the link I gave you can be used the same way.Deep Learning in a concrete caseFor this concrete case we'll try to de-noise the Cifar10 datasets using a neural network. 1 - prob / 2] = 0 return noisy_images def load_cifar10_with_noise(prob_salt_and_pepper): (x_train, _), (x_test, _) = cifar10.load_data() x_train_noisy = add_salt_and_pepper_noise(x_train, prob_salt_and_pepper) x_test_noisy = add_salt_and_pepper_noise(x_test, prob_salt_and_pepper) return (x_train_noisy/255, x_train/255), (x_test_noisy/255, x_test/255) prob_salt_and_pepper = 0.05 (X_train, y_train), (X_test, y_test) = load_cifar10_with_noise(prob_salt_and_pepper) plt.imshow(X_train[0]) plt.title('Image with salt-and-pepper noise') plt.axis('off') plt.show() plt.imshow(y_train[0]) plt.title('Originale Image without salt-and-pepper noise') plt.axis('off') plt.show() ]]>Unlike the previous chapter, the input will be noised images 32323 and the output un-noised images 32323, which means that we won't need the flatten() function to fully-connected the last hidden layer to the output layer.This neural network has a differents output than usual. This time the pixel themselves must be trained. Usually the hidden layers have this kind of shape :With the first hidden layer way wider than the input layer and gradually skrinking throughtout the layers to the output size. Keeping the same logic we have learned how to have 3d values input and 1d values output (colored images have 3 dimensions). Now we want to go from 3d input to 3d output. And because nothing shrinks between 3d and 3d we must expend the dimension on the first hidden layer. Using Conv2DTranspose we can expend it by 8 and then make it shrink back to 32323. We have normalized the pixels values by dividing them by 255, so now we can use the sigmoid function to make the output between 0 and 1. And after this we can plot the result :Unfortunately GridSearchCV disallow ConclusionNow you have finished this notion class you can supposedly create your own neural network. It's always tricky to create your very first model from scratch without help, so don't put too much pressure on yourself. Also remember that the more your task is complex the less you can expect your neural network to be perfect, the 100% accuracy or 0 of loss isn't possible for those cases. Last tips before your freedom :X and y dimensions must match the model.add(…) dimensions.Better use sigmoid for labelizing and softmax for classifying as the output activation.Use the right metric to express the loss, depending of what kind of neural network you want.input_shape is (w,h,1) for black and white images and (w,h,3) for colored images.Split the data into train and test to plot the loss and ensure they're no overfitting.