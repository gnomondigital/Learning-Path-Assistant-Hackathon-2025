Analysing Stock Market DataUse case showcasing the integration of Glue, PySpark, Athena, and S3 in the finance industry:AWS Glue featuresAWS Glue features fall into three major categories:Discover and organise dataTransform, prepare, and clean data for analysisBuild and monitor data pipelinesDiscover and organise dataUnify and search across multiple data stores – Store, index, and search across multiple data sources and sinks by cataloging all your data in AWS.Automatically discover data – Use AWS Glue crawlers to automatically infer schema information and integrate it into your AWS Glue Data Catalog.Manage schemas and permissions – Validate and control access to your databases and tables.Connect to a wide variety of data sources – Tap into multiple data sources, both on premises and on AWS, using AWS Glue connections to build your data lake.Transform, prepare, and clean data for analysisVisually transform data with a drag-and-drop interface – Define your ETL process in the drag-and-drop job editor and automatically generate the code to extract, transform, and load your data.Build complex ETL pipelines with simple job scheduling – Invoke AWS Glue jobs on a schedule, on demand, or based on an event.Clean and transform streaming data in transit – Enable continuous data consumption, and clean and transform it in transit. This makes it available for analysis in seconds in your target data store.Deduplicate and cleanse data with built-in machine learning – Clean and prepare your data for analysis without becoming a machine learning expert by using the FindMatches feature. This feature deduplicates and finds records that are imperfect matches for each other.Built-in job notebooks – AWS Glue Studio job notebooks provide serverless notebooks with minimal setup in AWS Glue Studio so you can get started quickly.Edit, debug, and test ETL code – With AWS Glue interactive sessions, you can interactively explore and prepare data. You can explore, experiment on, and process data interactively using the IDE or notebook of your choice.Define, detect, and remediate sensitive data – AWS Glue sensitive data detection lets you define, identify, and process sensitive data in your data pipeline and in your data lake.Build and monitor data pipelinesAutomatically scale based on workload – Dynamically scale resources up and down based on workload. This assigns workers to jobs only when needed.Automate jobs with event-based triggers – Start crawlers or AWS Glue jobs with event-based triggers, and design a chain of dependent jobs and crawlers.Run and monitor jobs – Run your AWS Glue jobs, and then monitor them with automated monitoring tools, the Apache Spark UI, AWS Glue job run insights, and AWS CloudTrail.Define workflows for ETL and integration activities – Define workflows for ETL and integration activities for multiple crawlers, jobs, and triggers.I- Data Ingestion:We receive daily stock market data in CSV format that you store in an AWS S3 bucket. The data includes information such as stock prices, volume traded, and market trends.Steps :Generate data:Necessary functions to upload data from yahoo to an S3 bucket : bool: """ This function uploads a file to an S3 bucket. Parameters: ---------- file_name: str File to upload bucket: str Bucket to upload to object_name: str S3 object name. If not specified then file_name is used Returns: - True if file was uploaded, else False """ if data is None: logging.error("There no data to Upload") pass try: csv_buffer = StringIO() data.to_csv(csv_buffer, index=True) s3_resource = boto3.resource("s3") s3_resource.Object(bucket, f"{folder_name}/{object_name}").put( Body=csv_buffer.getvalue() ) logger.info(f"File uploaded to s3://{bucket}/{object_name}/{object_name}") except ClientError as e: logging.error(f"Failed to upload file: {e}") return False return True ]]>Before launching the upload we need to create the S3 bucket. To do that we can use the CLI to create the S3 bucketCreate an S3 Bucket:Log into the AWS Management Console and navigate to the S3 service. Create a new S3 bucket to store the daily stock market data in CSV format.Create the S3 Bucket: Run the following command to create a new S3 bucket ]]>Replace with the desired name for your S3 bucket.Verify the S3 Bucket: Run the following command to list all S3 buckets in your account:The newly created S3 bucket should be listed in the output.By following these steps, you can successfully create an S3 bucket in AWS using the AWS CLI. You can now use the AWS CLI to upload data to the S3 bucket and perform other S3-related operations.Upload the Data to S3:Use the AWS Management Console, AWS CLI, or an S3 SDK to upload the daily stock market data to the S3 bucket. You can either upload the data manually or automate the process using scripts.Here's a step-by-step guide on how to upload data to an S3 bucket using the AWS CLI:Install the AWS CLI: Follow the instructions in the AWS documentation to install the AWS CLI on your local machine.Configure the AWS CLI: Run the aws configure command to set up your AWS credentials and default region.Upload the Data: Run the following command to upload the data file to the S3 bucket: s3:/// ]]>Replace with the path to the data file on your local machine, with the name of the S3 bucket, and with the desired key for the object in the S3 bucket.By following these steps, you can successfully upload data to an S3 bucket in AWS using the AWS CLI.Set up a Data Pipeline:If you have large volumes of data or need to automate the process of uploading data to S3, you can use AWS Data Pipeline to create a pipeline that transfers the data from its source to S3.For example to setup a job in EC2.In our case we launch the above code with the right configuration files :Verify Data in S3:Once the data has been uploaded to S3, verify that the data is accessible and has been uploaded correctly. You can use the AWS Management Console, AWS CLI, or an S3 SDK to view the contents of the S3 bucket.Verify the Upload: Run the following command to list the contents of the S3 bucket: ]]>The uploaded data file should be listed in the output.By following these steps, you can successfully ingest daily stock market data into an S3 bucket in AWS, ready for further processing and analysis.II- Data Cataloging:We will use AWS Glue to crawl the S3 bucket and create a data catalog of the stock market data.StepsAWS Glue is a fully managed ETL (extract, transform, load) service that makes it easy to move data between data stores. We will use AWS Glue to crawl an Amazon S3 bucket and create a data catalog of the stock market data, which will provide metadata information about the structure and schema of your dataTo do this, we'll first need to create an AWS Glue job, specifying the S3 bucket as the source and the data catalog as the target. Then, you'll run the job to start the crawling process, which will read the data in your S3 bucket, infer the schema and create metadata information in the data catalog. The data catalog will store information about the tables, columns, and data types of the stock market data in your S3 bucket, making it easier to query and analyze the data.Here is a step by step guide :Open the AWS Glue Console: Log in to the AWS Management Console and navigate to the AWS Glue service.Create a Glue database: Click on the "Databases" tab and then click the "Add database" button. Give your database a name and click "Create".Create a Glue Crawler: Click on the "Crawlers" tab and then click the "Add crawler" button. Give your crawler a name, select the S3 bucket containing your stock market data as the data source, and select the Glue database you created in step 3.2 as the target.Run the Crawler: Click "Finish" and then click "Run it now" to start the crawling process. AWS Glue will read the data in your S3 bucket, infer the schema and create metadata information in the Glue Data Catalog.Check the results: After the crawling process is complete, check the results in the Glue Data Catalog to make sure the metadata information about your stock market data has been correctly created.click on the "Tables" tab to open the Glue Data Catalog.Verify the table: Look for the table that corresponds to your stock market data and click on it to view its metadata information. Verify that the columns, data types, and schema match the data in your S3 bucket. The schema should match the data in your S3 bucket, including the columns "Date", "Open", "High", "Low", "Close", "Adj Close", and "Volume".Test the metadata: You can test the metadata information by using AWS Glue's built-in data catalog to run a SQL query on the stock market data stored in the S3 bucket. To do this, click on the "Query editor" tab and run a simple SELECT statement to retrieve some of the data. This will confirm that the metadata information in the Glue Data Catalog is correct and the data can be queried.Review the logs: If you encounter any errors during the crawling process, you can review the logs in the "History" tab to troubleshoot and resolve the issue.That's it! Now you have a Glue Data Catalog that provides metadata information about the structure and schema of your stock market data stored in an S3 bucket, making it easier to query and analyze the data.III- Data Transformation:We will use PySpark to write scripts to clean, transform, and standardize the stock market data. e.g. : We will use PySpark to calculate daily returns, moving averages, and volatility measures.StepsHere's a brief overview of how you can use PySpark to perform these calculations:Daily Returns: To calculate daily returns, we will use the formula (today's close price - yesterday's close price) / yesterday's close price. In PySpark, we can use the withColumn method to create a new column in our dataframe that contains the daily returns.Moving Averages: To calculate moving averages, we can take the average of a specific number of preceding or following days. In PySpark, we can use the rolling function from the pyspark.sql.functions module to perform this calculation.Volatility Measures: To calculate volatility measures such as standard deviation or historical volatility, we can use the stddev or var functions from the pyspark.sql.functions module.Here is a simple version of spark code to use in order to achieve the above calculus : 1) .dropna() .cache() ) date = ( df.select("_c0") .withColumnRenamed("_c0", "Date") .select( F.row_number().over(Window.orderBy(F.lit(1))).alias("row_number"), "*" ) .where(F.col("row_number") > 2) .dropna() .cache() ) close = date.join(close, on="row_number").drop("row_number").cache() returns = close.select( "Date", *[ (F.col(c) / F.lag(F.col(c)).over(Window.orderBy(F.lit(1))) - 1).alias( f"return_{c}" ) for c in close.columns[1:] ], ).cache() window_spec = Window.orderBy("Date").rowsBetween(-19, 0) moving_avg = close.select( "Date", *[F.avg(c).over(window_spec).alias(f"MA_{c}") for c in close.columns[1:]], ) volatility = returns.select( "Date", *[ F.stddev(c).over(window_spec).alias(f"volatility_{c}") for c in returns.columns[1:] ], ) folder_name_raw_data = config_dict["folder_name_transformed_data"] close.write.option("header", True).format("csv").mode("overwrite").parquet( f"s3://{bucket}/{folder_name_raw_data}/close" ) returns.write.option("header", True).format("csv").mode("overwrite").parquet( f"s3://{bucket}/{folder_name_raw_data}/returns" ) moving_avg.write.option("header", True).format("csv").mode("overwrite").parquet( f"s3://{bucket}/{folder_name_raw_data}/moving_avg" ) volatility.write.option("header", True).format("csv").mode("overwrite").parquet( f"s3://{bucket}/{folder_name_raw_data}/volatility" ) except Exception as e: # Log the error and raise an exception logger.error(f"Error processing data: {str(e)}") raise e finally: spark.stop() job = Job(glueContext) job.init(args["JOB_NAME"], args) job.commit() if __name__ == "__main__": args = getResolvedOptions(sys.argv, ["JOB_NAME"]) main(args) ]]>You can run this code in several ways on AWS:Amazon EMR: Amazon EMR is a managed cluster platform that makes it easy to process big data across a dynamic fleet of EC2 instances. You can launch an EMR cluster and run your PySpark code on the cluster. EMR automatically provisions the underlying infrastructure, making it a convenient option for running PySpark jobs.AWS Glue: AWS Glue is a fully managed ETL service that makes it easy to move data between data stores. You can write a PySpark script and run it as a Glue job. Glue provides built-in support for running PySpark scripts, so you can focus on writing your code without worrying about managing the underlying infrastructure.Log in to the AWS Management Console, and go to the AWS Glue service.In the left navigation menu, choose Jobs, and then choose Add Job.In the Add job page, enter a name for your job, choose a role that has the necessary permissions, and then choose Next.Choose A new script to be authored by you, and then choose Next.Choose Python as the script file type, and then choose your script file from your local machine.Choose Next, and then choose Finish.On the Jobs page, choose your newly created job, and then choose Action and Run Job.AWS Lambda: AWS Lambda is a serverless computing service that lets you run code without provisioning or managing servers. You can create a Lambda function and upload your PySpark code to it. You can then run your code on-demand in response to events, such as an incoming S3 file, making it a flexible option for running PySpark code.Each of these options has its own advantages and trade-offs, so we should choose the one that best fits our use case and requirements.IV- Automation with Terraform :To automate the building process we our going to launch our glue job from Terraform. We are going to use the following template :In this template we do provision :AWS S3 bucket to upload our source code, packages and necessary config file.AWS Glue job specification with all the necessary configurationsAWS IAM policy to use with Glue jobAWS Glue Trigger which allow us toIV- Data Analysis:We will use Athena to analyze the transformed stock market data stored in S3. You can run SQL queries in Athena to answer business questions such as:What is the average return for a stock over a given time period?Which stock has the highest volatility in a given time period?What is the overall market trend for a given time period?V- Data Visualization:We will visualize the results of your Athena queries using QuickSight to gain insights and make informed decisions. e.g. : We can create line charts, bar charts, or scatter plots to display the stock returns over time or compare the volatility of different stocks.By using this solution, we can quickly and easily process and analyze large volumes of stock market data to gain valuable insights and make informed investment decisions.Git Repo :You can find all the code used above in the following git repo : https://gitlab.gnomondigital.com/GlueDemoPrerequisites :Install the AWS CLI: Follow the instructions in the AWS documentation to install the AWS CLI on your local machine.Configure the AWS CLI: Run the aws configure command to set up your AWS credentials and default region.Thanks for reading, please follow us for more AWS content