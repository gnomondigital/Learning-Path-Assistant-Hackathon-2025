table of content : 16falsenonelisttrueIntroduction to data engineeringData engineering is a job that involves designing and building systems that can collect, store, and process large amounts of data. This data can come from many different sources, such as websites, sensors, or social media.Data pipelineA pipeline refers to a set of processes or tasks that are connected in a sequence in order to perform a particular task or set of tasks. These processes can involve collecting data from various sources, cleaning and organising the data, transforming it into a usable form, and then storing it for later use.Pipelines are often used to automate repetitive or time-consuming tasks, allowing data engineers to focus on more complex tasks and projects. For example, a data engineer might create a pipeline to automatically collect and clean data from a variety of sources on a regular basis, so that the data is always up-to-date and ready for analysis.Data lifecycleWe break the data engineering lifecycle into the following stages:GenerationStorageIngestionTransformationServing dataThe role of data engineering in the data pipelineThe role of data engineering in the data pipeline is to create and maintain the systems and infrastructure that allow data to be collected, stored, and processed efficiently and effectively.The skills and technologies used by data engineersCollecting data: Data engineers may use a variety of tools and technologies to collect data from different sources. This may include web scraping tools, APIs (Application Programming Interfaces), and messaging protocols like MQTT (Message Queue Telemetry Transport). They may also use programming languages like Python, Java, Scala to write custom code for data collection.Storing data: Data engineers use a variety of technologies to store data, including traditional relational databases like MySQL or PostgreSQL, as well as non-relational databases like MongoDB or Cassandra. They may also use data warehousing solutions like Amazon Redshift or Google BigQuery to store and analyse large amounts of data.Processing data: Data engineers use a variety of tools and technologies to process data in the pipeline. This may include ETL (extract, transform, load) software like Talend or Apache Beam, as well as programming languages like Python or Java to write custom code for data transformation and processing. They may also use distributed computing frameworks like Apache Spark or Hadoop to process large amounts of data in parallel.A data engineer has several top-level goals across the data lifecycle:  produce optimum ROI  reduce costs (financial and opportunity)  reduce risk (security, data quality)  maximise data value and utilityData storage and managementIntroduction to data storageYou need a place to store data.Choosing a storage solution is key to success in the rest of the data lifecycle, and it’s also one of the most complicated stages of the data lifecycle for a variety of reasons :First, data architectures in the cloud often leverage several storage solutions.Second, few data storage solutions function purely as storage, with many supporting complex transformation queries; even object storage solutions may support powerful query capabilities—e.g., Amazon S3 Select.Third, while storage is a stage of the data engineering lifecycle, it frequently touches on other stages, such as ingestion, transformation, and serving.Storage runs across the entire data engineering lifecycle, often occurring in multiple places in a data pipeline, with storage systems crossing over with source systems, ingestion, transformation, and serving.In many ways, the way data is stored impacts how it is used in all of the stages of the data engineering lifecycle.For example, cloud data warehouses can store data, process data in pipelines, and serve it to analysts. Streaming frameworks such as Apache Kafka and Pulsar can function simultaneously as ingestion, storage, and query systems for messages, with object storage being a standard layer for data transmission.Choices of databasesRelational databases: These are databases that store data in a structured format, using tables and relationships between the data. Examples include MySQL and PostgreSQL.Non-relational databases: These are databases that do not use a structured format like relational databases. They are often used for storing large amounts of data that is not easily modelled using tables and relationships. Examples include MongoDB and Cassandra.Data warehousing: These are specialised databases that are designed for fast querying and analysis of large amounts of data. They are often used in business intelligence applications. Examples include Amazon Redshift and Google BigQuery.File storage systems: These are systems that store data as files, rather than in a structured database. They are often used for storing large amounts of unstructured data, such as images or video. Examples include Amazon S3 and Google Cloud Storage.Setting up a data warehouseSetting up a data warehouse involves several steps:Identify the business requirements for the data warehouse. This includes determining the types of data that need to be stored in the warehouse, the users who will be accessing the data, and the types of queries and analysis that will be performed on the data.Design the data warehouse schema. This involves creating a logical model of the data that will be stored in the warehouse, including the relationships between different data entities and the dimensions and facts that will be used to analyse the data.Extract, transform, and load (ETL) the data. This involves extracting data from various sources, such as transactional databases and log files, transforming the data into a format that is suitable for analysis, and loading the data into the data warehouse.Set up access to the data warehouse. This includes creating roles and permissions for users, as well as setting up any necessary security measures.Test and optimise the data warehouse. This involves testing the performance of the data warehouse and making any necessary adjustments to improve its performance and scalability.Maintenance and updates. Ongoing maintenance and updates to the data warehouse will be necessary to ensure that it continues to meet the needs of the business and remains reliable over time.Data modelling and schema design A schema is a blueprint or plan that describes the structure of a database.It includes a list of all the data tables that are contained within the database, as well as the fields or columns that each table contains, and the relationships between different tables.A schema also defines the data types and constraints for each field, such as the maximum length of a text field or the range of values allowed for a numeric field.Good schema design is important for a number of reasons.It helps to ensure that data is organised in a way that is easy to understand and use, and it can also help to improve the performance of the database by minimising the amount of data that needs to be read or written when queries are executed.Evaluating storage systems:Here are a few key engineering questions to ask when choosing a storage system for a data warehouse, data lake-house, database, or object storage:Is this storage solution compatible with the architecture’s required write and read speeds?Will storage create a bottleneck for downstream processes?Do you understand how this storage technology works? Are you utilising the storage system optimally or committing unnatural acts? For instance, are you applying a high rate of random access updates in an object storage system? (This is an anti-pattern with significant performance overhead.)Will this storage system handle anticipated future scale? You should consider all capacity limits on the storage system: total available storage, read operation rate, write volume, etc.Will downstream users and processes be able to retrieve data in the required service-level agreement (SLA)?Are you capturing metadata about schema evolution, data flows, data lineage, and so forth? Metadata has a significant impact on the utility of data. Metadata represents an investment in the future, dramatically enhancing discoverability and institutional knowledge to streamline future projects and architecture changes.Is this a pure storage solution (object storage), or does it support complex query patterns (i.e., a cloud data warehouse)?Is the storage system schema-agnostic (object storage)? Flexible schema (Cassandra)? Enforced schema (a cloud data warehouse)?How are you tracking master data, golden records data quality, and data lineage for data governance? (We have more to say on these in “Data Management”.)How are you handling regulatory compliance and data sovereignty? For example, can you store your data in certain geographical locations but not others?Data ingestion and transformationData ingestion and transformation are two important processes in working with data. Data ingestion is the process of acquiring data from various sources and bringing it together in a single location. This can include data from databases, files, or even streaming data from the internet.Once the data is gathered, it often needs to be transformed to make it usable for analysis or other tasks. Data transformation involves cleaning, normalising, and shaping the data so that it can be used effectively. This can include tasks such as removing duplicate records, converting data into a consistent format, or aggregating data from multiple sources.Both data ingestion and transformation can be complex tasks, and often require specialised tools and technologies to manage the data effectively. For example, data ingestion may require the use of data integration tools to extract and move data from multiple sources. Data transformation may require the use of data quality tools to ensure that the data is accurate and complete.To effectively perform data ingestion and transformation, it is important to have a clear understanding of the data and its structure, as well as the desired outcome of the analysis or task. Additionally, it's important to monitor the data quality, and ensure that the data is accurate, complete and reliable.Setting up a data ingestion pipelineA data ingestion pipeline is a series of steps that are followed to acquire data from various sources and bring it into a central location for further processing and analysis. Setting up a data ingestion pipeline involves several key steps:Identify the data sources: The first step in setting up a data ingestion pipeline is to identify the sources of data that will be used. This could include databases, files, or streaming data from the internet.Extract the data: Once the sources of data have been identified, the next step is to extract the data from those sources. This could involve using data integration tools to extract data from databases, or writing code to scrape data from websites.Transform the data: After the data has been extracted, it often needs to be transformed to make it usable for further analysis or other tasks. This can include tasks such as cleaning, normalising, and shaping the data.Load the data: Once the data has been transformed, it is loaded into a central repository, such as a data warehouse or a cloud-based data storage system.Monitor and maintain the pipeline: Finally, it's important to monitor and maintain the pipeline to ensure that data is being ingested and transformed correctly. This includes monitoring for errors, and making adjustments to the pipeline as needed.An example of a data ingestion pipeline could be setting up a pipeline to gather data from social media platforms such as Twitter, Facebook, and Instagram. The pipeline would extract data from these platforms, transform it to remove duplicate records and irrelevant data, and load it into a data warehouse for further analysis.It's important to note that setting up a data ingestion pipeline can be complex and may require specialised tools and technologies to effectively manage the data. It's also necessary to ensure that the data quality is being monitored, and the pipeline is being maintained and optimised over time.Data transformation and cleansing techniquesData transformation and cleansing are important steps in working with data to make it usable for further analysis or other tasks. These techniques involve cleaning, formalising, and shaping the data so that it can be used effectively.Data cleansing is the process of identifying and correcting or removing inaccuracies, inconsistencies, and missing data in a dataset. This can include tasks such as removing duplicate records, filling in missing values, and correcting data that is in the wrong format.Data transformation involves changing the structure or format of the data to make it usable for further analysis. This can include tasks such as:Aggregating data from multiple sources to create a single datasetChanging the data into a consistent format, such as converting data into a common unit of measurementCreating new variables or columns based on existing dataPivoting data to change the format of the dataFiltering data to remove unwanted recordsEncoding categorical variablesSome examples of data transformation techniques include:Data normalisation: This technique is used to ensure that data is consistent across different columns or tables. It involves mapping data to a common scale or format.Data pivoting: This technique is used to change the format of the data, typically by turning rows into columns or vice versa.Data binning: This technique groups data into "bins" based on a certain value, such as age or income.Data encoding: This technique is used to represent categorical variables numerically so that they can be used in statistical models.It's important to note that data transformation and cleansing techniques can be complex and may require specialised tools and technologies to effectively manage the data. It's also necessary to ensure that the data quality is being monitored, and the pipeline is being maintained and optimised over time.Data analysisData analysis refers to the process of working with data to extract meaningful insights and information.This process can be broken down into several key steps:Data acquisition: This step involves gathering data from various sources and bringing it into a central location for further processing and analysis.Data preparation: This step involves cleaning, normalising, and shaping the data so that it can be used effectively. This includes removing inaccuracies, inconsistencies, and missing data as well as transforming data into a consistent format.Data analysis: This step involves using statistical and computational methods to extract insights and information from the data. This can include tasks such as identifying patterns, trends, and relationships within the data.Data visualisation: This step involves creating visual representations of the data, such as charts and graphs, to help make the insights and information more understandable and actionable.Data interpretation: Finally, this step involves interpreting the insights and information extracted from the data, in order to make informed decisions and predictions.Data analysis can be applied to a wide range of fields and industries, such as finance, healthcare, marketing, and more.For example, in finance, data processing and analysis can be used to identify patterns in stock prices, while in healthcare, data processing and analysis can be used to identify patterns in patient health data.Data analysis can be done using a variety of tools and technologies such as R, Python, SQL, SAS and others. These tools and technologies can be used to perform various types of analysis, such as descriptive, diagnostic, predictive and prescriptive analysis.It's important to note that data analysis is an iterative process, it often requires several iteration to refine the analysis and extract meaningful insights. Additionally, data quality and accuracy is crucial for effective analysis, and it's important to monitor and maintain the data throughout the process.Setting up a data processing platformSetting up a data processing platform involves several key steps:Identify the data sources: The first step in setting up a data processing platform is to identify the sources of data that will be used. This could include databases, files, or streaming data from the internet.Extract the data: Once the sources of data have been identified, the next step is to extract the data from those sources. This could involve using data integration tools to extract data from databases, or writing code to scrape data from websites.Ingest the data: After the data has been extracted, it needs to be ingested into the data processing platform. This could involve loading the data into a data warehouse or a cloud-based data storage system.Prepare the data: Once the data is loaded into the platform, it needs to be prepared for analysis. This can include tasks such as cleaning, normalising, and shaping the data.Analyse the data: Once the data is prepared, it can be analysed using various statistical and computational methods to extract insights and information. This can include tasks such as identifying patterns, trends, and relationships within the data.Visualise the data: The insights and information extracted from the data can be visualised using charts and graphs to make it more understandable and actionable.Monitor and maintain the platform: Finally, it's important to monitor and maintain the platform to ensure that data is being processed and analysed correctly. This includes monitoring for errors, and making adjustments to the platform as needed.An example of setting up a data processing platform could be setting up a platform to analyse customer data for a retail company. The platform would gather data from various sources such as sales data, customer demographic data, and website analytics. The data would be cleaned, normalised, and shaped, and then analysed to identify patterns and trends in customer behaviour. The insights would then be visualised and used to inform decisions about marketing, product development, and customer service.It's important to note that setting up a data processing platform can be complex and may require specialised tools and technologies to effectively manage the data. It's also necessary to ensure that the data quality is being monitored, and the platform is being maintained and optimised over time.Data analysis techniques and toolsData analysis is the process of using statistical and computational methods to extract insights and information from data. There are several techniques and tools that can be used to perform data analysis, including:Descriptive analysis: This technique involves summarising and describing the main characteristics of the data, such as the mean, median, and standard deviation. Tools such as R and Python can be used to perform descriptive analysis.Diagnostic analysis: This technique involves identifying the causes of specific behaviours or outcomes in the data. For example, it can be used to identify why a particular product is not selling well. Tools such as R and Python can be used to perform diagnostic analysis.Predictive analysis: This technique involves using historical data to make predictions about future events or outcomes. For example, it can be used to predict customer churn or stock prices. Tools such as R, Python, SAS and others can be used to perform predictive analysis.Prescriptive analysis: This technique involves using data and analytics to suggest actions or decisions that can be taken to improve a particular outcome. For example, it can be used to identify which marketing strategies are most effective. Tools such as R, Python, SAS and others can be used to perform prescriptive analysis.Data visualisation: This technique involves creating visual representations of the data, such as charts and graphs, to help make the insights and information more understandable and actionable. Tools such as Tableau, PowerBI, and MatPlotLib can be used to create data visualisations.Machine learning: This technique involves using algorithms and statistical models to analyse data and make predictions. It can be used for tasks such as image recognition, natural language processing, and predictive modelling. Tools such as TensorFlow, Scikit-learn and PyTorch can be used to perform Machine learning.