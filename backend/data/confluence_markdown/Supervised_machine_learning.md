IntroductionThis Learning Path will talk about Machine Learning. At the end of this class you’ll be able to teach anything to your computer. We’re gonna use Python’s librairie scikit-learn. To install it run this command in your terminal :Then to ensure it’s correctly installed run this in a Python file :If you have an error ModuleNotFoundError: No module named 'sklearn' while executing this line refers to that website : https://scikit-learn.org/stable/install.html The goal of Machine Learning is make a computer predict automatically an answer by only reading a few features. Scikit-Learn is a Python package which gives us all the tools we need to process, train and predict our dataset.Supervised RegressionLet’s imagine you’re a baker who sells bread at a price based on the weight, you could have a spreadsheet like this:Weight (entrance)Price (exit)10g$0.7530g$2.1050g$3.8870g$6.10100g$8.88Now if you have to sell a 65g bread you now have to predict a price for that bread. And that’s what could do Machine Learning for you. Down below you have the above situation transferred in a .py file :The class neigh has been trained with the table of table weight to predict the content of the table price. And after training it the computer can predict price of a weight he has never seen before.Machine Learning is basically training a class with an input of one or several columns of a spreadsheet to predict an exit column.Important NoteKNNRegressor is a prediction algorithm among plenly of different available with sklearn, choose wisely the one who fits the most to your situation (see chapter: Hyperparameter gestion) and which value to set in your parameter (like n_neighbors), and also all of those algorithm models are using the same functions “fit”, “predict” etc…Same situation but now we take in consideration the length of the bread to determine the price. Here is a spreadsheet we have, we would like to determine the price of a 40g and 22cm bread.Weight (entrance)Length (entrance)Price (exit)10g8cm$0.7530g15cm$2.1050g32cm$3.8870g56cm$6.10100g73cm$8.8840g22cm$???The python file would be basically the same. This time we’re gonna have a slightly different input.As a convention the entrance columns are called X in python and the exit is called y.Now let’s looks closely to the predict function I just used :I can update it to make it predict several prices of bread. As a result neigh.predict will give me a table of my result :The predict function can predict as much as you want as long as you respect the good shape of input you used in the training.As you just guessed, you can also use float numbers as an input. And the output function could be the result of an arithmetic calcul, that’s what we call regression. A regression prediction model give you an estimation of something you could get with a calculator. And it’s called Supervised Regression because we give the computer the solution (the price column) so he can work on, but we’ll see soon that you can predict without knowing yourself what’s the exit.Why did the computer predicted a 40g, 22cm and a 45g, 27cm bread as the same price ?Because our model hasn't trained enough, we need to give him more spreadsheet line to make some better prediction. The more our situation is complex the more we need input. As an example if you wanna train your model to determine how positive or negative are tweets you need to train it on at least 10 000 tweets.Supervised ClassificationSupervised classification is similar to Supervised regression. The difference is what we put in X and y. If regression predict calculation, classification predict what class would the entrance belongs to.Let’s say we want to determine if an animal is an ostrich, a wolf, an eagle or a bat, depending of if it has fur or feathers and is flying or not. We could have a spreadsheet like this :type_of_skincan_flyspecies0 (Fur)1 (Yes)3 (Bat)1 (Feather)1 (Yes)2 (Eagle)0 (Fur)0 (No)1 (Wolf)0 (Fur)1 (Yes)3 (Bat)1 (Feather)0 (No)0 (Ostrich)0 (Fur)0 (No)1 (Wolf)1 (Feather)1 (Yes)2 (Eagle)As you can imagine a computer cannot interpret words so we are symbolizing class’s name with number. And with our new table we can build a script to predict the species of a new animal which we only know if it flies or not and its type of skin.Here is the script that predict the above table :Just like a regressor, neigh.predict return a table for every input. But this time you cannot describe a class with a float number.Note : You can represent your class with the integer you want I choose 0,1,2,3 to represent my four animals but I could use any other integer.How does KNeighbors works and what is n_neighbors ?KNeighborsClassifier (aka KNN Classifier) uses the mathematical concept of k-nearest neighbors algorithm to predict a value. You don’t truly need to understand how KNN works to use the scikit-learn package, but here is a quick explanation for your culture :Explanation of knn : Figure 1, Figure 2, Figure 3Explanation of knn : Figure 1, Figure 2, Figure 3You have a set of data (figure 1) with the feature Y-Axis and X-Axis to classify them in two category A and B. And now you have a new value to classify (purple point in the graphe), KNN algorithm is a solution to determine if purple point is a A or a B.To determine in which class belongs purple point be take based on the Euclidian distance the k nearest points from purple point (figure 2), in this example we take a certain number of his closest neighbors (in that case 7) from the purple points, and we proceed to a popular vote to determine the predicted class. For this dataset within the 7 nearest neighbors of purple point, we have more Class B than A, so purple is a B.A variant of KNeighbors is RadiusNeighbors (figure 3), with this model we select all the points within a certain radius to proceed to a popular vote.KNeighborsClassifier predict class using this algorithm, and the parameter n_neighbors is the number of points who’ll proceed to the vote.Supervised Machine Learning in a Concrete caseIn this concrete case we’re gonna work on a dataset of iris, a purple flower and as a lot of flowers they have sepals and petals.Picture of Iris flowers by Christina BrinzaFrom sklearn we can import a dataset to train ourself on it. It contains four entrance and one exit. Here is how to load this dataset, we have our X and our y. Our goal is to predict what is the species of an incoming unknown iris based on the length and width of its sepal and petal.Out : [5.1 3.5 1.4 0.2] 0Out : [5.9 3.2 4.8 1.8] 1Out : [6.9 3.2 5.7 2.3] 2Here is a table of what it could looks like with 3 lines among the 150 others :sepal_lengthsepal_widthpetal_lengthpetal_widthspecies5.1 cm3.5 cm1.4 cm0.2 cm0 (setosa)5.9 cm3.2 cm4.8 cm1.8 cm1 (versicolor)6.9 cm3.2 cm5.7 cm2.3 cm2 (virginica)Now we can train our model to predict the class depending of float values :Out : 0You can mix float and integer inputs however you want. The only thing you need to worry about is which of type of librairie should you use. It’s very simple use a Classifier or a Regressor depending of what you want as an exit.Hyperparameter gestionFrom the beginning of this learning path we’ve been working with KNeighborsClassifier or KNeighborsRegressor. Those are two prediction model among the other. Indeed sklearn gives us many prediction model to predict with, they all work on the same way. Here is an example script of what I'm talking about :Out : According to KNeighbors, the price of a 40g and 22cm bread is $2.99 Out : According to RandomForest, the price of a 40g and 22cm bread is $2.33 Out : According to SVR, the price of a 40g and 22cm bread is $3.05 Out : According to MLP, the price of a 40g and 22cm bread is $2.95As you can see those models are predicting similar results. You can find other models and many more information about supervised learning on this website:Supervised learningMaybe you have noticed the parameters of my models : n_neighbors, max_depth, random_state etc… Those are the hyper-parameters of the prediction models. Changing them would make the prediction more accurate or not. To make good and reliable prediction we need to choose wisely our prediction model and our hyper-parameters.For example KNN as 8 parameters that you can interact with :class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, ***, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)Visit scikit-learn website to view the details.If we take the 10 first lines of our iris dataset and verify the differences between y and the prediction of X, we can see that the model neigh made some errors while predicting :Out : [ True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True False True True True True True True True True True True True True False True True True True True True True True True True True True True False True True True True True True True True True True True True True True True True]In that it means that our models sometimes doesn’t predict the good species of a flower. A very useful fonction of sklearn gives us the accuracy of the model we’ve instantiated :Out : 0.98Which means that neigh predict the good species of an iris based on its sepal and petal dimension 98% of the time. And by optimizing the hyper-parameters we can make that percent higher.Luckily you don't have to randomly search for the best combo of parameters, a class named GridSearchCV can do it for you. And it looks like this :Out : {'n_neighbors': 10, 'weights': 'distance'}In the above example I tried to determine whether combo of value for n_neighbors between 1,2,5,7 & 10 and weights between ‘uniform’ and ‘distance’, would give the more accurate result.As a reminder you can find the details of whatever value you can put as a parameter for KNeighborsClassifier on this website:sklearn.neighbors.KNeighborsClassifierAnd GridSearchCV has decreed that a model KNeighborsClassifier('n_neighbors': 10, 'weights': 'distance') would have the best results. Let’s try this :Out : 1.0As you can see by choosing wisely our parameter neigh is now predicting well 100% percent of the time.Now you know how you manage your hyper-parameters. But it would be great to not overfit our model, it’ll be our next chapter.Overfitting and train test splitThe overfitting is the most common problem when you’re doing machine or deep learning. It occurs when you train too much your data on a dataset. The below image is a good illustration :Illustration of the Overfitting with the green lineThis image shows two types of points blue and red, and its goal is to predict a line that would scatter the blue from the red points. If you train your model to generate this line it’ll generate the black line, but if you train it too much it will generate the green line. The problem with the green line’s model is that it will only be able to predict precisely this dataset of points and will be lost if you try it on a new one. Unlike the black one that has a few errors but will probably predict well another dataset.The most common way to prevent a model to overfit is to split your data in two around 70%-30%. We train our model on 70% of the original dataset and test it on the remaining 30%. By doing that the model will have to predict values it never encountered before, which will highly lower the chance it overfit.This image shows you what we just talked about :Illustration of splitting data into test and train datasetAnd hopefully for us, sklearn has a function that split the data for us :Out: Length of the original dataset: 150Out: Length of the train dataset: 105Out: Length of the test dataset: 45We have now a train dataset to train our data with (using .fit() function) and a test dataset to test our data by using value the computer never trained on (using .predict() function). Splitting the dataset into train and test datasets is mandatory when practicing Machine Learning.To be more specific, you cannot claim your model is accurate if you test it with the data you used for training.After you have split your data you can predict.ConclusionWith this course you can now, predict data if you have an entrance and an exit. Also remember this :X must be a list of list and y a list, model.predict() will be a list.Choose between Classifier or Regressor depending of what kind of exit you need.Choose which model algorithm will fit better (KNeirestNeighbor, RandomForest, etc…).After when you want to make it more efficient split your dataset in two.Predict your answer, the answer will be determinate using different way depending of which model you have chosen.Use GridSearchCV to automatically search the optimal hyper-parameter.You have finished this class about Supervised Machine Learning, congratulation !Here is just for you a list of a few model you can use for your Machine Learning work and how it works, note that the explanation is based on how the classification model work.Full nameRegressorClassifierLinkHow does it predictK Nearest NeigborsKNeighborsRegressorKNeighborsClassifierhttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html Select the k nearest neighbors for a point to predict and proceed to a popular vote.Radius NeighborsRadiusNeighborsRegressorRadiusNeighborsClassifierhttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor Select all the neighbors within a radius and proceed to a popular vote.Decision TreeDecisionTreeRegressorDecisionTreeClassifierhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html Create branches and under-branches for every features to determine in which class belongs a new set.Random ForestRandomForestRegressorRandomForestClassifierhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html Create a forest of decision tree with random different weights and make them vote.Support VectorSVRSVChttps://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html Split the plan using a curve (linear or not) to split the dataset with vector calculus.Multi-Layer-PerceptronMLPRegressorMLPClassifierhttps://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor Create a neural network where the data will come across and will reset the weight by back propagation to make it fit the exit. (it’s deep learning, but you better directly use a keras model)RegressionLinearRegressionLogisticRegressionhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html Calculate the probability it belongs to a class using the sigmoid function