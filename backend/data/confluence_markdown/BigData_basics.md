 Table of contents :noneWhat is Big Data? Data within Big Data environments generally accumulates from being amassed within the enterprise via applications, sensors and external sources. Data processed by a Big Data solution can be used by enterprise applications directly or can be fed into a data warehouse to enrich existing data there. The results obtained through the processing of Big Data can lead to a wide range of insights and benefits, such as:operational optimizationactionable intelligenceidentification of new marketsaccurate predictionsfault and fraud detectionmore detailed recordsimproved decision-makingscientific discoveries0.TerminologyDatasets : group of data sharing the same attributes.Data Analysis :process of examining data to find facts, relationships, patterns, insights and/or trends.it’s done to help make better decisionsexample: how much ice cream a store should order in relation to weather forecast information.Data Analytics :used for : descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics.Characteristics of Big Datavolume : Organisations and users world-wide create over 2.5 EBs of data a day. As a point of comparison, the Library of Congress currently holds more than 300 TBs of data.where did this data come from ? online transactions, experiments, sensors, social media…Velocity:The velocity of data translates into the amount of time it takes for the data to be processed once it enters the enterprise’s perimeter.Examples of high-velocity Big Data datasets produced every minute include tweets, video, emails and GBs generated from a jet engine.Variety :refers to the multiple formats and types of data that need to be supported by Big Data solutions.Veracity :Refers to the quality or fidelity of data.Data that enters Big Data environments needs to be assessed for quality, which can lead to data processing activities to resolve invalid data and remove noise.  Noise is data that cannot be converted into information and thus has no value, whereas signals have value and lead to meaningful information. Data with a high signal-to-noise ratio has more veracity than data with a lower ratio.Value :Value is defined as the usefulness of data for an enterprise.Value is dependent on how long data processing takes because analytics results have a shelf-life; for example, a 20 minute delayed stock quote has little to no value for making a trade compared to a quote that is 20 milliseconds old.Data that has high veracity and can be analysed quickly has more value to a business.Technologies used for Big Data  (these informations are dated to 2016 and gives the ground of Big Data, know that in 2023 it’s different)This chapter discusses the enterprise technologies that support the transformation of data into information, information into knowledge and knowledge into wisdom.This chapter covers the following topics:Online Transaction Processing (OLTP)It’s a software to process online transactionsOnline transaction means that it’s treated real-time and it’s not batch processedIt generate a structured data that is used for analyticsThis system supports simple queries : insert, delete, updateExample : ticket reservation, banking …Online Analytical Processing (OLAP)Used for processing data analysis queriesCan serve as data source and data sink to receive dataSupports fast reporting capabilityExtract Transform Load (ETL)Process of loading data from a source system into a target system.The required data is first obtained or extracted from the sources, after which the extracts are modified or transformed by the application of rules. Finally, the data is inserted or loaded into the target system.Data WarehousesIt is a central, enterprise-wide repository consisting of historical and current data.Batch jobs periodically load data into a data warehouse from operational systems like ERP, CRM and SCM.ERP : enterprise resource planning. CRM : customer relationship management. SCM : supply chain managementWith periodic data imports from across the enterprise, the amount of data contained in a given data warehouse will continue to increase.Over time this leads to slower query response times for data analysis tasks.To resolve this shortcoming, data warehouses usually contain optimised databases, called analytical databases, to handle reporting and data analysis tasks.An analytical database can exist as a separate DBMS, as in the case of an OLAP database.Data MartsA data mart is a subset of the data stored in a data warehouse that typically belongs to a department, division, or specific line of business.OLTP : online transaction processingTraditional BIUtilises descriptive and diagnostic analytics to provide information on historical and current events. It is not “intelligent” because it only provides answers to correctly formulated questions.It consists ofAd-hoc reports :manually processing data to produce custom-made reportsDashboards:The information displayed on dashboards is generated at periodic intervals in realtime or near-realtime.The source of data is Data Warehouses and Data Marts that contains correct informationsBig Data BIWhile traditional BI analyses generally focus on individual business processes, Big Data BI analyses focus on multiple business processes simultaneously.Helps reveal patterns and anomalies across a broader scope within the enterprise.Next-generation data warehouse establishes a standardised data access layer across a range of data sources.Big Data processing conceptsparallel data processing :Involves the simultaneous execution of multiple sub-tasks that collectively comprise a larger task.The goal is to reduce the execution time by dividing a single larger task into multiple smaller tasks that run concurrently.parallel data processing can be achieved through multiple networked machines.But it is more typically achieved within the confines of a single machine with multiple processors or coresdistributed data processing :Similar to parallel processing that it uses “divide-and-conquer”.It’s achieved through physically separated machines.Here is an example of distributed data processing.Hadoop :Hadoop is an open-source framework for large-scale data storage and data processing.It can be used as an ETL engine or as an analytics engine for processing large amounts of structured, semi-structured and unstructured data.Hadoop implements the MapReduce processing framework.processing workloads :Workloads is the amount and nature of data that is processed within a certain amount of time. We have two types:Batch :also known as offline processing, involves processing data in batches and usually imposes delaysQueries can be complex and involve multiple joins.OLAP systems commonly process workloads in batches.Transactional :Transactional processing is also known as online processingData is processed interactively without delay, resulting in low-latency responses.OLTP and operational systems, which are generally write-intensive, fall within this category.cluster :Clusters provide necessary support to create horizontally scalable storage solutions.Since clusters are highly scalable, they provide an ideal environment for Big Data processing as large datasets can be divided into smaller datasets and then processed in parallel in a distributed manner.Applications of Big DataBusiness intelligence and analytics: Companies use big data to gain insights into customer behaviour, market trends, and operational performance. They can use this information to make data-driven decisions that improve their bottom line.Healthcare: Big data is used to improve patient outcomes by analysing patient data, such as medical records, clinical trials, and genomic data. It can help healthcare providers identify patterns and trends that can lead to better treatment and care.Finance: Big data is used in finance to detect fraudulent activities, identify market trends, and make investment decisions. It is also used to analyse customer behaviour and improve customer experience.Manufacturing: Big data is used to improve production efficiency, reduce waste, and improve product quality. It can also help manufacturers predict demand and optimise supply chains.Transportation: Big data is used to optimise transportation networks, reduce traffic congestion, and improve safety. It is also used in logistics to track shipments and optimise delivery routes.Government: Big data is used by governments to improve public services, monitor public health, and track criminal activities. It is also used to analyse social media data to identify trends and public opinions.Summary : Big data is a term used to describe data sets that are too large and complex to be processed using traditional methods. The main characteristics of big data are volume, variety, and velocity, and it can be sourced from a wide range of sources. Big data has become increasingly important in many industries and domains, and it is expected to continue to grow in importance in the future. The storage and processing of big data require specialised technologies and tools. HDFS (Hadoop Distributed File System), Apache Spark, and NoSQL databases are some of the popular technologies used in big data.These technologies are designed to handle large volumes of data, provide high availability, and support distributed processing.