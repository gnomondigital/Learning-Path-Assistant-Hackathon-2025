 Table of contents :What is Apache Spark?Apache Spark is an Open source analytical processing engine for large scale powerful distributed data processing and machine learning applications. It’s a cluster computing system that provides high-level API in Java, Scala, Python and R. It can access data from HDFS, Cassandra, HBase , Hive, Tachyon, and any Hadoop data source.Spark runs in Standalone, YARN and Mesos cluster manager.Apache Spark FeaturesIn-memory computationDistributed processing using parallelizeFault-tolerantImmutablelazy evaluationCache and persistenceInbuilt-optimization when using DataFramesApache Spark AdvantagesApplications running on Spark are 100x faster than traditional systems.Using Spark we can process data from Hadoop HDFS, AWS S3, Databricks DBFS, Azure Blob Storage, and many file systems.Spark is a general-purpose, in-memory, fault-tolerant, distributed processing engine that allows you to process data efficiently in a distributed fashion.Spark also is used to process real-time data using Streaming and Kafka.Using Spark Streaming you can also stream files from the file system and also stream from the socket.Spark natively has machine learning and graph libraries.Provides connectors to store the data in NoSQL databases like MongoDBApache Spark EcosystemApache Spark Core APIAll the functionalities provided by Apache Spark are built on top of Spark Core. It delivers speed by providing in-memory computation capability. Thus Spark Core is the foundation of parallel and distributed processing of huge datasets.The key features of Apache Spark Core are:It is in charge of essential I/O functionalities.Significant in programming and observing the role of the Spark cluster.Task dispatching.Fault recovery.It overcomes the snag of **MapReduce** by using in-memory computation.Spark Core is embedded with a special collection called RDD (resilient distributed dataset). RDD is among the abstractions of Spark. Spark RDD handles partitioning data across all the nodes in a cluster. It holds them in the memory pool of the cluster as a single unit. There are two operations performed on RDDs: Transformation and ActionTransformation: It is a function that produces new RDD from the existing RDDs.Action: In Transformation, RDDs are created from each other. But when we want to work with the actual dataset, then, at that point we use Action.Apache Spark SQLThe Spark SQL component is a distributed framework for structured data processing. Using Spark SQL, Spark gets more information about the structure of data and the computation. With this information, Spark can perform extra optimization. It uses same execution engine while computing an output. It does not depend on API/ language to express the computation.Spark SQL works to access structured and semi-structured information. It also enables powerful, interactive, analytical application across both streaming and historical data. Spark SQL is Spark module for structured data processing. Thus, it acts as a distributed SQL query engine.Features of Spark SQL include:Cost based optimizer.Mid query fault-tolerance: This is done by scaling thousands of nodes and multi-hour queries using the Spark engine.Full compatibility with existing **Hive** data.DataFrames and SQL provide a common way to access a variety of data sources. It includes Hive, Avro, Parquet, ORC, JSON, and JDBC.Provision to carry structured data inside Spark programs, using either SQL or a familiar Data Frame APIApache Spark StreamingIt is an add-on to core Spark API which allows scalable, high-throughput, fault-tolerant stream processing of live data streams. Spark can access data from sources like Kafka, Flume, Kinesis or TCP socket. It can operate using various algorithms. Finally, the data so received is given to file system, databases and live dashboards. Spark uses Microbatching for real-time streaming. Micro-batching is a technique that allows a process or task to treat a stream as a sequence of small batches of data. Hence Spark Streaming, groups the live data into small batches. It then delivers it to the batch system for processing. It also provides fault tolerance characteristics.There are 3 phases of Spark Streaming:a. GATHERING The Spark Streaming provides two categories of built-in streaming sources:**Basic sources: ****These are the sources which are available in the StreamingContext API. Examples: file systems, and socket connections.Advanced **sources: ****These are the sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. Hence Spark access data from different sources like Kafka, Flume, Kinesis, or TCP sockets.b. PROCESSING The gathered data is processed using complex algorithms expressed with a high-level function. For example, map, reduce, join and window.c. DATA STORAGE The Processed data is pushed out to file systems, databases, and live dashboards. Spark Streaming also provides high-level abstraction. It is known as discretized stream or DStream. DStream in Spark signifies continuous stream of data. We can form DStream in two ways either from sources such as Kafka, Flume, and Kinesis or by high-level operations on other DStreams. Thus, DStream is internally a sequence of RDDs.Apache Spark MLlibMLlib in Spark is a scalable Machine learning library that discusses both high-quality algorithm and high speed.The motive behind MLlib creation is to make machine learning scalable and easy. It contains machine learning libraries that have an implementation of various machine learning algorithms like clustering, regression, classification and collaborative filtering.Some lower level machine learning primitives like generic gradient descent optimization algorithm are also present in MLlib.In Spark Version 2.0 the RDD-based API in spark.mllib package entered in maintenance mode. In this release, the DataFrame-based API is the primary Machine Learning API. So, from now MLlib will not add any new feature to the RDD based API.The reason MLlib is switching to DataFrame-based API is that it is more user-friendly than RDD. Some of the benefits of using DataFrames are it includes Spark Data sources, SQL DataFrame queries Tungsten and Catalyst optimizations, and uniform APIs across languages. MLlib also uses the linear algebra package Breeze.Breeze is a collection of libraries for numerical computing and machine learning.Apache Spark GraphXGraphX in Spark is API for graphs and graph parallel execution. It is network graph analytics engine and data store.Furthermore, GraphX extends Spark RDD by bringing in light a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge.GraphX also optimizes the way in which we can represent vertex and edges when they are primitive data types. To support graph computation it supports fundamental operators (e.g., subgraph, join Vertices, and aggregate Messages) as well as an optimized variant of the Pregel API.Apache Spark ArchitectureApache Spark works in a master-slave architecture where the master is called “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.When you enter your code in spark, SparkContext in the driver program creates the job when we call an Action. This job submits to DAG Scheduler which creates the operator graph and then submits it to task Scheduler. Task Scheduler launches the task via cluster manager. Thus, with the help of a cluster manager, a Spark Application is launched on a set of machines.The driver runs in its own Java process. and each executor is also a separate java process.Apache Spark DriverThe main() method of the program runs in the driver. The driver is the process that runs the user code that creates RDDs, and performs transformation and action, and also creates SparkContext. When the Spark Shell is launched, this signifies that we have created a driver program. On the termination of the driver, the application is finished.The driver program splits the Spark application into tasks and schedules them to run on the executor. The task scheduler resides in the driver and distributes task among workers. The two main key roles of a driver are:Converting user program into the task.Scheduling task on the executor.The structure of Spark program at a higher level is: RDDs consist of some input data, derive new RDD from existing using various transformations, and then after it performs an action to compute data. In Spark Program, the DAG (directed acyclic graph) of operations is created implicitly. And when the driver runs, it converts that Spark DAG into a physical execution plan.Apache Spark ClusterSpark relies on the cluster manager to launch executors . It is a pluggable component in Spark. On the cluster manager, jobs and actions within a spark application are scheduled by Spark Scheduler in a FIFO fashion. Alternatively, the scheduling can also be done in Round Robin fashion. The resources used by a Spark application can dynamically adjust based on the workload. Thus, the application can free unused resources and request them again when there is demand. This is available on all coarse-grained cluster managers, i.e. standalone mode, YARN mode, and Mesos coarse-grained mode.Apache Spark ExecutorsAn individual task in the given Spark job runs in the Spark executors. Executors are launched once in the beginning of a Spark Application and then they run for the entire lifetime of the application. Even if the Spark executor fails, the Spark application can continue with ease. There are two main roles of the executors:Runs the task that makes up the application and returns the result to the driver.Provide in-memory storage for RDDs that are cached by the user.Apache Spark ContextSparkContext is the heart of Spark Application. It establishes a connection to the Spark Execution environment. It is used to create Spark RDDs, accumulators, and broadcast variables, access Spark services and run jobs. SparkContext is a client of Spark execution environment and acts as the master of Spark application. The main works of Spark Context are:Getting the current status of spark applicationCanceling the jobCanceling the StageRunning job synchronouslyRunning job asynchronouslyAccessing persistent RDDUnpersisting RDDProgrammable dynamic allocationPrior to spark 2.0, you can create SparkContext by programmatically using its constructor, and pass parameters like master and appName at least as these are mandatory params. The below example creates context with a master as local and app name as Spark_Example_App.You can also create it using SparkContext.getOrCreate(). It actually returns an existing active SparkContext otherwise creates one with a specified master and app name.Note that the string “local” is added to run in local mode. In practice, when running on a cluster, you will not want to hardcode masterin the program, but rather launch the application with spark-submitand receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process.If you want to create SparkContext, first SparkConf should be made. The SparkConf has a configuration parameter that our Spark driver application will pass to SparkContext. Some of these parameter defines properties of Spark driver application. While some are used by Spark to allocate resources on the cluster, like the number, memory size, and cores used by executor running on the worker nodes.Apache Spark SessionSince Spark 2.0, SparkSession has become an entry point to Spark to work with RDD, DataFrame, and Dataset. Prior to 2.0, SparkContext used to be an entry point.SparkSession was introduced in version Spark 2.0, It is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame, and DataSet. SparkSession’s object *spark*is the default variable available in spark-shell and it can be created programmatically using SparkSessionbuilder pattern.Prior to Spark 2.0, SparkContext used to be an entry point, and it’s not been completely replaced with SparkSession, many features of SparkContext are still available and used in Spark 2.0 and later. You should also know that SparkSession internally creates SparkConfig and SparkContext with the configuration provided with SparkSession.Spark Session also includes all the APIs available in different contexts:SparkContextSQLContextStreamingContextHiveContextRDD (Resilient Distributed Datasets)In Spark, an RDD (Resilient Distributed Dataset) is a fundamental data structure that represents a collection of data elements that can be processed in parallel across a cluster of computers. RDDs are immutable and can be partitioned across the nodes of a cluster, allowing for distributed processing of large data sets.RDDs are created from data that is either loaded from external storage or generated from transformations on existing RDDs. Once an RDD is created, it can be processed using various operations, such as filtering, mapping, reducing, and joining, to transform the data into a desired format.RDDs provide several advantages, including fault tolerance, scalability, and the ability to cache data in memory for faster processing. They are also the foundation for higher-level data processing frameworks in Spark, such as DataFrames and Datasets.There are three ways to create RDDs in Spark:Data in stable storageOther RDDsParallelizing an already existing collection in the driver program. One can also operate Spark RDDs in parallel with a low-level API that offers transformations and actions.Spark RDD can also be cached and manually partitioned. Caching is beneficial when we use RDD several times. And manual partitioning is important to correctly balance partitions. Generally, smaller partitions allow distributing RDD data more equally, among more executors. Hence, fewer partitions make the work easy.Programmers can also call a persist method to indicate which RDDs they want to reuse in future operations. Spark keeps persistent RDDs in memory by default, but it can spill them to disk if there is not enough RAM. Users can also request other persistence strategies, such as storing the RDD only on disk or replicating it across machines, through flags to persist.For example, here is how to create a parallelized collection holding the numbers 1 to 5:Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list. We describe operations on distributed datasets later on.One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows: distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b).Basic Example:To illustrate RDD basics, consider the simple program below:The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness. Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.If we also wanted to use lineLengths again later, we could add:before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed.RDD operationsRDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.Transformations: (examples)TransformationMeaningmap(func)Return a new distributed dataset formed by passing each element of the source through a function func.filter(func)Return a new dataset formed by selecting those elements of the source on which func returns true.flatMap(func)Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).mapPartitions(func)Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator => Iterator when running on an RDD of type T.groupByKey([numPartitions])When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable) pairs.Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks.reduceByKey(func, [numPartitions])When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.sortByKey([ascending], [numPartitions])When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.coalesce(numPartitions)Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.repartition(numPartitions)Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.Actions:ActionMeaningreduce(func)Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.collect()Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.count()Return the number of elements in the dataset.first()Return the first element of the dataset (similar to take(1)).take(n)Return an array with the first n elements of the dataset.Check the spark documentation for a complete list of operations.RDD lineage and DAGDAG and RDD lineage are related concepts in Apache Spark, but they represent different aspects of the Spark execution model.RDD lineage refers to the sequence of transformations applied to an RDD to create a new RDD. Since RDDs are immutable, every transformation creates a new RDD that depends on the previous RDD in the lineage. RDD lineage is used by Spark to provide fault tolerance: if a node fails, Spark can recreate lost partitions by replaying the transformations in the lineage.On the other hand, a DAG (Directed Acyclic Graph) represents the logical structure of a Spark job, which consists of a sequence of transformations on one or more RDDs. The DAG captures the dependencies between the RDDs and the transformations that are applied to them. The DAG is used by Spark to optimize the execution of the job, by grouping the transformations into stages and scheduling them for execution on the cluster.In other words, RDD lineage is a low-level concept that represents the dependencies between individual RDDs, while DAG is a high-level concept that represents the logical structure of a Spark job, including the dependencies between multiple RDDs and transformations. RDD lineage is used by Spark to provide fault tolerance, while DAG is used by Spark to optimize the execution of a job for performance.DAG schedularA DAG scheduler is a component in Apache Spark that is responsible for scheduling the tasks in a Spark job based on the Directed Acyclic Graph (DAG) of transformations and dependencies between RDDs.The DAG scheduler takes the logical execution plan of a Spark job, which is represented as a DAG of stages, and converts it into a physical plan that can be executed on a cluster of machines. It does this by first breaking the job into stages, which are sets of transformations that can be executed together without shuffling data between machines. The stages are then further divided into tasks, which are the smallest units of work that can be executed on a single executor.The DAG scheduler also performs various optimizations to improve the performance of the job, such as pipelining together certain transformations to reduce the amount of data shuffling needed between stages.Once the DAG scheduler has generated the physical plan, it hands it off to the task scheduler, which is responsible for actually executing the tasks on the available resources in the cluster.Overall, the DAG scheduler plays a critical role in the performance of Spark jobs by determining the optimal execution plan for the job based on the dependencies between RDDs and transformations.How it worksUsing spark-submit, the user submits an application.In spark-submit, we invoke the main() method that the user specifies. It also launches the driver program.The driver program asks for the resources to the cluster manager that we need to launch executors.The cluster manager launches executors on behalf of the driver program.The driver process runs with the help of user application. Based on the actions and transformation on RDDs, the driver sends work to executors in the form of tasks.The executors process the task and the result sends back to the driver through the cluster manager.