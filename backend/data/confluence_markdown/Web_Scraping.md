IntroductionWeb scraping is the practice of retrieving from an internet website its source code to extract from it data. Many IT languages are used to scrap, but for this class we'll learn scraping from Python and it's librairy bs4. Run this command in your Python terminal:This Learning Path will teach you how to scrap data, what is legal and what isn't, and how to create a database from the data.First of all, using Chrome or Firefox, if you right click on the browser and select “inspect” you can checkout the source code of the page this is what we'll try to retrieve.Source code of https://en.wikipedia.org/wiki/Main_Page Safari doesn't have the feature so I advise you to use Chrome or Firefox for your scraping activities. Also not that Python Basics are mandatory to follow this lesson.First scrapingLet's try to scrap something simple : https://guthib.com/ is a joke website with a very simple source code :Our goal is to retrieve the sentence You spelled it wrong using a python script, if you checkout the source code you'll see that the sentence is stored in the html tag . So let's just request bs4 and the package BeautifulSoup to give us the text of the first tag of the websites.") soup = BeautifulSoup(response.text, "html.parser") print(soup) ]]>Note that soup contain all the source code of the page "https://guthib.com/", now let's find the line :") soup = BeautifulSoup(response.text, "html.parser") print(soup.find("h1")) print(soup.find("h1").get_text()) ]]>The method find return the first tag you ask for. And the method get_text will return the text of the whatever “find” has found. Now let's try on a true website.Scrap a WebsiteNow let's try to scrap and get a value buried in a very long source code, right click/ inspect on precisely the text you seek and it will shows you where is it in the source code.In addition of searching by the name of the html tag you can search something using the class or the id. All of the print in the code below refers to the same text but using different ways to describe what to find :") soup = BeautifulSoup(response.text, "html.parser") print(soup.find("h1")) print(soup.find(id="firstHeading")) print(soup.find(class_="firstHeading mw-first-heading")) print(soup.find("h1", class_="firstHeading mw-first-heading")) print(soup.find(class_="mw-page-title-main")) print(soup.find("span", class_="mw-page-title-main")) ]]>All of those print are looking for the Wikipedia page's title and they all print the same text if we had get_text.") soup = BeautifulSoup(response.text, "html.parser") print(soup.find("h1").get_text()) print(soup.find(class_="firstHeading mw-first-heading").get_text()) print(soup.find("h1", class_="firstHeading mw-first-heading").get_text()) print(soup.find(class_="mw-page-title-main").get_text()) print(soup.find("span", class_="mw-page-title-main").get_text()) print(soup.find(id="firstHeading").get_text()) ]]>Now let's increase the difficulty by extracting text from a tag in another tag in another tag etc. For exemple the “Web scraping” article is stored in the **mw-body-content mw-content-ltr and every paragraph is stored in a . Let's try to scrap the first paragraph. To be sure to find the right .") soup = BeautifulSoup(response.text, "html.parser") print(soup.find("div", class_="mw-body-content mw-content-ltr").find("p").get_text()) ]]>And now let's try to retrieve all the 's text of the **mw-body-content mw-content-ltr using find_all that will return a list of all the tag that match the description :") soup = BeautifulSoup(response.text, "html.parser") content = soup.find("div", class_="mw-body-content mw-content-ltr") for paragraph in content.find_all("p"): print(paragraph.get_text()) ]]>Also an important note is that if you type the tag class's name wrong, find() will return None and find().get_text() will raise an error because they're no text to get from None.Scrap can be long, use tqdm package to create loading bars.Pro tip:When you copy and paste a class name from your browser to your python framework, sometimes you'll paste the wrong name. As you just saw spaces are allowed in a class name and in some websites (which I suspect to prevent scraping), you will paste for example : “ mw-body-content mw-content-ltr “ with two spaces between content and mw and at the beginning and the end, instead of “mw-body-content mw-content-lt”. So if your python script doesn't find your tag ensure the parameter class_ doesn't have double spaces or spaces at the beginning or the end.Scraping limitsLet's try to connect to the Indeed page of Airbus : https://fr.indeed.com/cmp/Airbus-Group , as you can see you have access to the source code :Indeed page of the Airbus GroupBut when you request the source code this happens, i put the soup variable in a html emulator to show you which website you retreived :") soup = BeautifulSoup(response.text, "html.parser") print(soup) ]]>Indeed, http://Indeed.com like thousands of websites forbid scraping, to prevent massive scam, and theft of personal data. Your request is redirected to an error page. To check out what is allow or disallow on a websites, we can visit the page robots.txt. This page is available on every websites and give details of what is allowed and what isn't (for example : https://en.wikipedia.org/robots.txt, https://fr.indeed.com/robots.txt, https://www.google.com/robots.txt). It shows every blacklisted bots, and which page a random bot can visit. Also note that websites can disallow a bot using robot protection even if the robots.txt technically allow it. Checkout the official documentation here : http://www.robotstxt.org/orig.html .Also remember that you can only get data hard coded in the html page, in dynamics websites which use javascript function to shows informations, scraping is inefficient.You may now have understood that Web Scraping can become illegal depending of which data you're trying to have access to. The first rules of legal scraping are as follows :The data must be collected only for your company’s purpose and not made publicThe data must not cause financial or reputational losses to its ownersDon't scrap names or emails without consentSource : https://scrapfly.io/is-web-scraping-legal And also don't mass request a website or they will ban your IP address.Web scraping in a concrete caseNow you master the basics of Web Scraping let's try to create our first database using Web Scraping. For this chapter we will scrap the French bank Boursorama stock information (which is allowed). We're gonna try to scrap the main informations of the 9 pages of the following page.Page 1 of https://www.boursorama.com/bourse/actions/cotations/ Source code of https://www.boursorama.com/bourse/actions/cotations/ As you can see all the rows are in the “c-block” (u-relative would works too). So let's scrap all the within the div named c-block.") soup = BeautifulSoup(response.text, "html.parser") c_block = soup.find("div", class_="c-block") for tr in c_block.find_all("tr"): print(tr.get_text()) ]]>Scraping the text of the in one returned the rows labels and all the columns values joined together. We need to go deeper and get the text of all the of all the of the c-block .") soup = BeautifulSoup(response.text, "html.parser") c_block = soup.find("div", class_="c-block") for tr in c_block.find_all("tr", class_="c-table__row"): for td in tr.find_all("td"): print(td.get_text()) ]]>And now we have all the data we needed let's create a true pipeline extractor from the previous code. We'll use pandas to create a dataframe. Type in your terminal :") soup = BeautifulSoup(response.text, "html.parser") c_block = soup.find("div", class_="c-block") for tr in c_block.find_all("tr", class_="c-table__row"): row = [td.get_text() for td in tr.find_all("td")] dataset.append(row) if row!=[] else None print(pd.DataFrame(dataset)) ]]>We have created a Dataframe with all the values we needed, now let's clean the row of the spaces and the \n and add customs columns names.") soup = BeautifulSoup(response.text, "html.parser") c_block = soup.find("div", class_="c-block") for tr in c_block.find_all("tr", class_="c-table__row"): row = [cleaned(td) for td in tr.find_all("td")] dataset.append(row) if row!=[] else None print(pd.DataFrame(dataset, columns=columns)) ]]>And now we have the main 203 stock values of France in our Dataframe. Feel free to retrieve the url link of each stock and request each pages source code to get even more informations.ConclusionWeb Scraping is a powerful method to build a database using open-data ressources online. Remember to always look the source code (Firefox or Chrome not Safari), to find the good html tags to use. Ensure to never collect neither personal data nor data when it's disallowed.And to paraphrase J.R.R. Tolkien : “Web Scraping is power, power corrupt, so don't let it corrupt you or the FBI might create a fellowship to cast you in the lava”.