Definition"Great Expectations" is a Python library for data validation and documentation. It helps data practitioners to define, document, and test data pipelines to ensure data quality and consistency.Great Expectations go through a checklist to make sure the data passes all these tests before being used.Why use great expectationsCatch data quality issues early: Great Expectations allows you to define expectations (i.e. rules) for your data, which can help you catch data quality issues early in your data pipelines. By setting expectations for things like data types, value ranges, and data structure, you can quickly detect issues like missing data, invalid values, or data schema mismatches.Improve data documentation: Great Expectations provides a way to document your data expectations and share them with other members of your team. By documenting your expectations in code, you can ensure that everyone is on the same page about what the data should look like and how it should be used.Simplify data testing: Great Expectations provides a simple way to test your data pipelines and ensure that they are working correctly. By running tests that validate your data expectations, you can catch issues early and avoid downstream problems.Integrate with other data tools: Great Expectations can be integrated with other data tools like Apache Airflow, Databricks, and dbt, making it easy to incorporate data validation and testing into your existing workflows.Integrate seamlessly with DAG execution tools: Great Expectations integrates seamlessly with DAG execution tools such as Airflow, dbt, Prefect, Dagster, Kedro, etc.Getting StartedIn this section, I will show you how to get started with Great Expectations and answer some questions you might have about this tool.To install Great Expectations, type:to make sure that you have installed it run the command :Next we need to initialise it :you will have the following result :once you responded with yes the following folder will be created in your directory :Great expectation structureThe files :The file great_expectations.yml contains the main configuration of your deployment.The directory expections stores all your Expectations as JSON files.The directory plugins holds code for any custom plugins you might have.The directory uncommitted contains files that shouldnâ€™t be in version control.Use the CLI to:Run great_expectations datasource new to connect to your data.Run great_expectations checkpoint new to bundle data with Expectation Suite(s) in a Checkpoint for later re-validation.Run great_expectations suite --help to create, edit, list, profile Expectation Suites.Run great_expectations docs --help to build and manage Data Docs sites.Edit your configuration in great_expectations.ymlto:Move Stores to the cloudAdd Slack notifications, PagerDuty alerts, etc.Customise your Data DocsGreat Expectations WorkflowGreat Expectations generally includes 3 steps: connect to data, create expectations, and validate your data.Connect to Data :Download data about Google Stock Price: Daily, Weekly & Monthly (2023) from kaggleGoogle Stock Price: Daily, Weekly & Monthly (2023)then store them in a data folder named data/, then enter the command to make the connection:We can either connect Great Expectations to the files on our system or SQL. Here we chose to:connect it with files on our systemprocess our files with Pandasspecify data to be the directory that stores all of our data files.After answering all of these questions, a notebook will be automatically created for you! This notebook allows you to configure your new Datasource. Datasource tells Great Expectations where your data lives and how to get it.